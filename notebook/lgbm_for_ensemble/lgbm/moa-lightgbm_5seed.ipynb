{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T08:49:16.734718Z",
     "start_time": "2020-11-17T08:49:15.722431Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import re\n",
    "import math\n",
    "import pickle\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.multioutput import MultiOutputClassifier, ClassifierChain\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T08:49:16.739705Z",
     "start_time": "2020-11-17T08:49:16.736713Z"
    },
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random as rn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "    rn.seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T08:49:16.830167Z",
     "start_time": "2020-11-17T08:49:16.740702Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "def score(Y, Y_pred):\n",
    "    _, n_classes = Y.shape\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for j in range(n_classes):\n",
    "        loss = log_loss(Y.iloc[:, j], Y_pred.iloc[:, j], labels=[0, 1])\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T08:49:16.873459Z",
     "start_time": "2020-11-17T08:49:16.831164Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../input/iterativestratification')\n",
    "\n",
    "import numpy as np\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.model_selection._split import _BaseKFold\n",
    "\n",
    "\n",
    "class MultilabelGroupStratifiedKFold(_BaseKFold):\n",
    "    def __init__(self, n_splits=5, random_state=None, shuffle=False):\n",
    "        super().__init__(n_splits=n_splits, random_state=random_state, shuffle=shuffle)\n",
    "\n",
    "    def _iter_test_indices(self, X=None, y=None, groups=None):\n",
    "        cv = MultilabelStratifiedKFold(\n",
    "            n_splits=self.n_splits,\n",
    "            random_state=self.random_state,\n",
    "            shuffle=self.shuffle,\n",
    "        )\n",
    "\n",
    "        value_counts = groups.value_counts()\n",
    "        regular_index = value_counts.loc[\n",
    "            (value_counts == 6) | (value_counts == 12) | (value_counts == 18)\n",
    "        ].index.sort_values()\n",
    "        irregular_index = value_counts.loc[\n",
    "            (value_counts != 6) & (value_counts != 12) & (value_counts != 18)\n",
    "        ].index.sort_values()\n",
    "\n",
    "        group_to_fold = {}\n",
    "        tmp = Y.groupby(groups).mean().loc[regular_index]\n",
    "\n",
    "        for fold, (_, test) in enumerate(cv.split(tmp, tmp)):\n",
    "            group_to_fold.update({group: fold for group in tmp.index[test]})\n",
    "\n",
    "        sample_to_fold = {}\n",
    "        tmp = Y.loc[groups.isin(irregular_index)]\n",
    "\n",
    "        for fold, (_, test) in enumerate(cv.split(tmp, tmp)):\n",
    "            sample_to_fold.update({sample: fold for sample in tmp.index[test]})\n",
    "\n",
    "        folds = groups.map(group_to_fold)\n",
    "        is_na = folds.isna()\n",
    "        folds[is_na] = folds[is_na].index.map(sample_to_fold).values\n",
    "\n",
    "        for i in range(self.n_splits):\n",
    "            yield np.where(folds == i)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T08:49:16.933745Z",
     "start_time": "2020-11-17T08:49:16.874456Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "\n",
    "class ClippedFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, copy=True, high=0.99, low=0.01):\n",
    "        self.copy = copy\n",
    "        self.high = high\n",
    "        self.low = low\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.data_max_ = X.quantile(q=self.high)\n",
    "        self.data_min_ = X.quantile(q=self.low)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.copy:\n",
    "            X = X.copy()\n",
    "\n",
    "        X.clip(self.data_min_, self.data_max_, axis=1, inplace=True)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T08:49:16.995579Z",
     "start_time": "2020-11-17T08:49:16.934743Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def compute_row_statistics(X, prefix=\"\"):\n",
    "    Xt = pd.DataFrame()\n",
    "\n",
    "    for agg_func in [\n",
    "        # \"min\",\n",
    "        # \"max\",\n",
    "        \"mean\",\n",
    "        \"std\",\n",
    "        \"kurtosis\",\n",
    "        \"skew\",\n",
    "    ]:\n",
    "        Xt[f\"{prefix}{agg_func}\"] = X.agg(agg_func, axis=1)\n",
    "\n",
    "    return Xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T08:49:17.272838Z",
     "start_time": "2020-11-17T08:49:16.996577Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def display_importances(\n",
    "    importance_df, png_path=f\"feature_importance.png\",\n",
    "):\n",
    "    \"\"\"feature_importance plot\"\"\"\n",
    "    importance_df.sort_values(by=\"importance\", ascending=False).to_csv(f\"feature_importance.csv\")\n",
    "    cols = (\n",
    "        importance_df[[\"feature\", \"importance\"]]\n",
    "        .groupby(\"feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:100]\n",
    "        .index\n",
    "    )\n",
    "    best_features = importance_df.loc[importance_df.feature.isin(cols)]\n",
    "    plt.figure(figsize=(8, 15))\n",
    "    sns.barplot(\n",
    "        x=\"importance\",\n",
    "        y=\"feature\",\n",
    "        data=best_features.sort_values(by=\"importance\", ascending=False),\n",
    "    )\n",
    "    plt.title(\"LightGBM (avg over folds)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(png_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T08:49:17.277825Z",
     "start_time": "2020-11-17T08:49:17.274833Z"
    }
   },
   "outputs": [],
   "source": [
    "#dtype = {\"cp_type\": \"category\", \"cp_dose\": \"category\"}\n",
    "#index_col = \"sig_id\"\n",
    "#\n",
    "#train_features = pd.read_csv(\n",
    "#   \"../input/lish-moa/train_features.csv\", dtype=dtype, index_col=index_col\n",
    "#)\n",
    "#X = train_features.select_dtypes(\"number\")\n",
    "#Y_nonscored = pd.read_csv(\n",
    "#   \"../input/lish-moa/train_targets_nonscored.csv\", index_col=index_col\n",
    "#)\n",
    "#Y = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\", index_col=index_col)\n",
    "#groups = pd.read_csv(\n",
    "#   \"../input/lish-moa/train_drug.csv\", index_col=index_col, squeeze=True\n",
    "#)\n",
    "#\n",
    "#columns = Y.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T08:49:20.883080Z",
     "start_time": "2020-11-17T08:49:17.278822Z"
    }
   },
   "outputs": [],
   "source": [
    "dtype = {\"cp_type\": \"category\", \"cp_dose\": \"category\"}\n",
    "index_col = \"sig_id\"\n",
    "\n",
    "sys.path.append(\n",
    "    r\"C:\\Users\\81908\\jupyter_notebook\\poetry_work\\tfgpu\\01_MoA_compe\\code\"\n",
    ")\n",
    "import datasets\n",
    "\n",
    "DATADIR = datasets.DATADIR\n",
    "\n",
    "groups = pd.read_csv(f\"{DATADIR}/train_drug.csv\", dtype=dtype, index_col=index_col, squeeze=True)\n",
    "train_features = pd.read_csv(f\"{DATADIR}/train_features.csv\", dtype=dtype, index_col=index_col)\n",
    "#X_test = pd.read_csv(f\"{DATADIR}/test_features.csv\", dtype=dtype, index_col=index_col)\n",
    "X = train_features.select_dtypes(\"number\")\n",
    "Y_nonscored = pd.read_csv(f\"{DATADIR}/train_targets_nonscored.csv\", index_col=index_col)\n",
    "Y = pd.read_csv(f\"{DATADIR}/train_targets_scored.csv\", index_col=index_col)\n",
    "\n",
    "columns = Y.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T08:49:23.282902Z",
     "start_time": "2020-11-17T08:49:20.883080Z"
    }
   },
   "outputs": [],
   "source": [
    "clipped_features = ClippedFeatures()\n",
    "X = clipped_features.fit_transform(X)\n",
    "\n",
    "with open(\"clipped_features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(clipped_features, f)\n",
    "# アンサンブルのために統計値, nonscoredは入れない \n",
    "#c_prefix = \"c-\"\n",
    "#g_prefix = \"g-\"\n",
    "#c_columns = X.columns.str.startswith(c_prefix)\n",
    "#g_columns = X.columns.str.startswith(g_prefix)\n",
    "#X_c = compute_row_statistics(X.loc[:, c_columns], prefix=c_prefix)\n",
    "#X_g = compute_row_statistics(X.loc[:, g_columns], prefix=g_prefix)\n",
    "#X = pd.concat([X, X_c, X_g], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T08:49:23.288886Z",
     "start_time": "2020-11-17T08:49:23.283899Z"
    }
   },
   "outputs": [],
   "source": [
    "is_drug_cv = True\n",
    "n_splits = 5\n",
    "n_seeds = 5\n",
    "# LBS = 0.0008  # ラベルスムージングは全然効かないからやめる\n",
    "LBS = 0.0\n",
    "\n",
    "params = {\n",
    "    \"num_leaves\": 2,\n",
    "    \"max_depth\": 1,\n",
    "    \"min_data_in_leaf\": 969,\n",
    "    \"objective\": \"binary\",\n",
    "    \"learning_rate\": 0.01,\n",
    "}\n",
    "num_boost_round = 10000\n",
    "verbose_eval = 300\n",
    "early_stopping_rounds = verbose_eval\n",
    "\n",
    "#DEBUG = True\n",
    "DEBUG = False\n",
    "if DEBUG:\n",
    "    columns = [\n",
    "        \"atp-sensitive_potassium_channel_antagonist\",  # 陽性ラベル1個だけ\n",
    "        \"erbb2_inhibitor\",  # 陽性ラベル1個だけ\n",
    "        \"antiarrhythmic\",  # 陽性ラベル6個だけ\n",
    "#        \"aldehyde_dehydrogenase_inhibitor\",  # 陽性ラベル7個だけ\n",
    "#        \"lipase_inhibitor\",  # 陽性ラベル12個だけ\n",
    "#        \"sphingosine_receptor_agonist\",  # 陽性ラベル25個だけ\n",
    "#        \"igf-1_inhibitor\",  # 陽性ラベル37個だけ\n",
    "#        \"potassium_channel_activator\",  # 陽性ラベル55個だけ\n",
    "#        \"potassium_channel_antagonist\",  # 陽性ラベル98個だけ\n",
    "#        \"dopamine_receptor_agonist\",  # 陽性ラベル121個だけ\n",
    "#        \"nfkb_inhibitor\",  # 陽性ラベル832個\n",
    "#        \"cyclooxygenase_inhibitor\",  # 陽性ラベル435個\n",
    "#        \"dna_inhibitor\",  # 陽性ラベル402個\n",
    "#        \"glutamate_receptor_antagonist\",  # 陽性ラベル367個\n",
    "#        \"tubulin_inhibitor\",  # 陽性ラベル316個\n",
    "#        \"pdgfr_inhibitor\",  # 陽性ラベル297個\n",
    "#        \"calcium_channel_blocker\",  # 陽性ラベル281個\n",
    "#        \"flt3_inhibitor\",  # 陽性ラベル279個\n",
    "#        \"progesterone_receptor_agonist\",  # 陽性ラベル119個\n",
    "#        \"hdac_inhibitor\",  # 陽性ラベル106個\n",
    "    ]\n",
    "    Y = Y[columns]\n",
    "    \n",
    "    params[\"n_estimators\"] = 2\n",
    "    n_splits = 4\n",
    "    num_boost_round = 50\n",
    "    verbose_eval = 5\n",
    "    early_stopping_rounds = verbose_eval\n",
    "    print(f\"DEBUG: {DEBUG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T08:49:23.292875Z",
     "start_time": "2020-11-17T08:49:23.289884Z"
    }
   },
   "outputs": [],
   "source": [
    "train_size, n_features = X.shape\n",
    "_, n_classes_nonscored = Y_nonscored.shape\n",
    "_, n_classes = Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T08:49:37.708712Z",
     "start_time": "2020-11-17T08:49:23.293872Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------ fold:0 ------------\n",
      "[LightGBM] [Info] Number of positive: 10, number of negative: 19012\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.152874 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 222364\n",
      "[LightGBM] [Info] Number of data points in the train set: 19022, number of used features: 873\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000526 -> initscore=-7.550241\n",
      "[LightGBM] [Info] Start training from score -7.550241\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's binary_logloss: 0.00252526\tvalid_1's binary_logloss: 0.0107158\n",
      "[600]\ttraining's binary_logloss: 0.00151945\tvalid_1's binary_logloss: 0.0104838\n",
      "Early stopping, best iteration is:\n",
      "[528]\ttraining's binary_logloss: 0.00172104\tvalid_1's binary_logloss: 0.0104423\n",
      "\n",
      "------------ fold:1 ------------\n",
      "[LightGBM] [Info] Number of positive: 16, number of negative: 19030\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.165293 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 222364\n",
      "[LightGBM] [Info] Number of data points in the train set: 19046, number of used features: 873\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000840 -> initscore=-7.081183\n",
      "[LightGBM] [Info] Start training from score -7.081183\n",
      "Training until validation scores don't improve for 300 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:14, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    250\u001b[0m                                     evaluation_result_list=None))\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m         \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   2368\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__set_objective_to_none\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2369\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot update due to null objective function.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2370\u001b[1;33m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0m\u001b[0;32m   2371\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2372\u001b[0m                 ctypes.byref(is_finished)))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "counts = np.empty((n_seeds * len(columns) * n_splits))\n",
    "\n",
    "f_importance = np.zeros((n_features,))\n",
    "Y_pred = np.zeros((train_size, n_classes))\n",
    "Y_pred = pd.DataFrame(Y_pred, columns=Y.columns, index=Y.index)\n",
    "\n",
    "for i in range(n_seeds):\n",
    "    set_seed(seed=i)\n",
    "\n",
    "    for tar_i, tar_col in tqdm(enumerate(Y.columns)):\n",
    "        Y_target = Y[[tar_col]]\n",
    "\n",
    "        if is_drug_cv:\n",
    "            cv = MultilabelGroupStratifiedKFold(\n",
    "                n_splits=n_splits, random_state=i, shuffle=True\n",
    "            )\n",
    "            cv_split = cv.split(X, Y_target, groups)\n",
    "        else:\n",
    "            StratifiedKFold(n_splits=n_splits, random_state=i, shuffle=True)\n",
    "            cv_split = cv.split(X, Y_target)\n",
    "\n",
    "        for j, (trn_idx, val_idx) in enumerate(cv_split):\n",
    "            print(f\"\\n------------ fold:{j} ------------\")\n",
    "            counts[i * len(columns) + tar_i * n_splits + j] = Y_target.iloc[trn_idx].sum()\n",
    "            \n",
    "            X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n",
    "            Y_train, Y_val = Y_target.iloc[trn_idx], Y_target.iloc[val_idx]\n",
    "\n",
    "            # Label Smoothing. https://www.kaggle.com/gogo827jz/self-stacking-groupcv-xgboost\n",
    "            Y_train = Y_train * (1 - LBS) + 0.5 * LBS\n",
    "            \n",
    "            lgb_train = lgb.Dataset(X_train, Y_train)\n",
    "            lgb_eval = lgb.Dataset(X_val, Y_val, reference=lgb_train)\n",
    "            \n",
    "            model = lgb.train(\n",
    "                params,\n",
    "                lgb_train,\n",
    "                valid_sets=[lgb_train, lgb_eval],\n",
    "                verbose_eval=verbose_eval,\n",
    "                num_boost_round=num_boost_round,\n",
    "                early_stopping_rounds=early_stopping_rounds,\n",
    "            )\n",
    "            Y_pred[tar_col][val_idx] += model.predict(X_val, num_iteration=model.best_iteration) / n_seeds\n",
    "            \n",
    "            f_importance += np.array(model.feature_importance(importance_type=\"gain\")) / (n_seeds * n_splits)\n",
    "            \n",
    "            joblib.dump(model, f\"model_seed_{i}_fold_{j}_{Y.columns[tar_i]}.jlb\", compress=True)\n",
    "\n",
    "importance_df = pd.DataFrame({\"feature\": model.feature_name(), \"importance\": f_importance})\n",
    "            \n",
    "Y_pred[train_features[\"cp_type\"] == \"ctl_vehicle\"] = 0.0\n",
    "\n",
    "with open(\"counts.pkl\", \"wb\") as f:\n",
    "    pickle.dump(counts, f)\n",
    "\n",
    "with open(\"Y_pred.pkl\", \"wb\") as f:\n",
    "    pickle.dump(Y_pred[columns], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T08:49:38.702623Z",
     "start_time": "2020-11-17T08:49:37.709711Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11855409667033498"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(Y[columns], Y_pred[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T08:49:38.712596Z",
     "start_time": "2020-11-17T08:49:38.703620Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'importance_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-b73c2144e51b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdisplay_importances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimportance_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'importance_df' is not defined"
     ]
    }
   ],
   "source": [
    "display_importances(importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Platt Scaling\n",
    "Train a Logistic Regression model to calibrate the results\n",
    "- https://www.kaggle.com/gogo827jz/kernel-logistic-regression-one-for-206-targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T08:49:38.713594Z",
     "start_time": "2020-11-17T08:49:15.808Z"
    }
   },
   "outputs": [],
   "source": [
    "# predict_probaでだしたY_predをロジスティク回帰で確率に補正する\n",
    "\n",
    "counts = np.empty((n_classes))\n",
    "\n",
    "X_new = Y_pred.values\n",
    "Y_cali = Y_pred.copy()\n",
    "\n",
    "for tar in tqdm(range(Y.shape[1])):\n",
    "    \n",
    "    targets = Y.values[:, tar]\n",
    "    X_targets = X_new[:, tar]\n",
    "    counts[tar] = targets.sum()\n",
    "\n",
    "    if targets.sum() >= n_splits:\n",
    "        \n",
    "        Y_cali[Y.columns[tar]] = np.zeros((Y_cali.shape[0], ))\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=n_splits, random_state=0, shuffle=True)\n",
    "\n",
    "        for n, (tr, te) in enumerate(skf.split(targets, targets)):\n",
    "            x_tr, x_val = X_targets[tr].reshape(-1, 1), X_targets[te].reshape(-1, 1)\n",
    "            y_tr, y_val = targets[tr], targets[te]\n",
    "\n",
    "            model = LogisticRegression(penalty=\"none\", max_iter=1000)\n",
    "            model.fit(x_tr, y_tr)\n",
    "            Y_cali[Y.columns[tar]].iloc[te] += model.predict_proba(x_val)[:, 1]\n",
    "            \n",
    "            joblib.dump(model, f\"calibrate_model_target_{Y.columns[tar]}.jlb\", compress=True)\n",
    "\n",
    "with open(\"counts_calibrate.pkl\", \"wb\") as f:\n",
    "    pickle.dump(counts, f)\n",
    "\n",
    "with open(\"Y_pred_calibrate.pkl\", \"wb\") as f:\n",
    "    pickle.dump(Y_cali[columns], f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T08:49:38.714591Z",
     "start_time": "2020-11-17T08:49:15.811Z"
    }
   },
   "outputs": [],
   "source": [
    "score(Y[columns], Y_cali[columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pkl check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T08:49:38.714591Z",
     "start_time": "2020-11-17T08:49:15.812Z"
    }
   },
   "outputs": [],
   "source": [
    "path = r\"counts.pkl\"\n",
    "with open(path, 'rb') as f:\n",
    "    counts = pickle.load(f)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T08:49:38.715588Z",
     "start_time": "2020-11-17T08:49:15.814Z"
    }
   },
   "outputs": [],
   "source": [
    "path = r\"counts_calibrate.pkl\"\n",
    "with open(path, 'rb') as f:\n",
    "    counts = pickle.load(f)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T08:49:38.716586Z",
     "start_time": "2020-11-17T08:49:15.816Z"
    }
   },
   "outputs": [],
   "source": [
    "path = r\"Y_pred.pkl\"\n",
    "with open(path, 'rb') as f:\n",
    "    Y_pred = pickle.load(f)\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T08:49:38.717583Z",
     "start_time": "2020-11-17T08:49:15.819Z"
    }
   },
   "outputs": [],
   "source": [
    "path = r\"Y_pred_calibrate.pkl\"\n",
    "with open(path, 'rb') as f:\n",
    "    Y_pred = pickle.load(f)\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T08:49:38.717583Z",
     "start_time": "2020-11-17T08:49:15.820Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pathlib\n",
    "\n",
    "test_features = pd.read_csv(\n",
    "    \"../input/lish-moa/test_features.csv\", dtype=dtype, index_col=index_col\n",
    ")\n",
    "X_test = test_features.select_dtypes(\"number\")\n",
    "\n",
    "\n",
    "with open(\"./clipped_features.pkl\", \"rb\") as f:\n",
    "    clipped_features = pickle.load(f)\n",
    "X_test = clipped_features.transform(X_test)\n",
    "# アンサンブルのため統計値, nonscoredは入れない \n",
    "#X_c = compute_row_statistics(X_test.loc[:, c_columns], prefix=c_prefix)\n",
    "#X_g = compute_row_statistics(X_test.loc[:, g_columns], prefix=g_prefix)\n",
    "#X_test = pd.concat([X_test, X_c, X_g], axis=1)\n",
    "\n",
    "\n",
    "# lgbで予測\n",
    "Y_test_pred = np.zeros((X_test.shape[0], len(columns)))\n",
    "Y_test_pred = pd.DataFrame(Y_test_pred, columns=columns, index=test_features.index)\n",
    "for target in columns:\n",
    "    model_paths = glob.glob(f\"./model_seed_*_{target}.jlb\")\n",
    "    for model_path in model_paths:\n",
    "        model = joblib.load(model_path)\n",
    "        Y_test_pred[target] += model.predict(X_test) / len(model_paths)\n",
    "print(Y_test_pred.shape)\n",
    "display(Y_test_pred)\n",
    "\n",
    "\n",
    "# lgbの予測値補正\n",
    "model_paths = glob.glob(f\"./calibrate_model_target_*.jlb\")\n",
    "for model_path in model_paths:\n",
    "    target = str(pathlib.Path(model_path).stem).replace(\"calibrate_model_target_\", \"\")\n",
    "\n",
    "    if target in columns:\n",
    "        # print(target)\n",
    "        model = joblib.load(model_path)\n",
    "        X_targets = Y_test_pred.loc[:, target].values.reshape(-1, 1)\n",
    "        Y_test_pred.loc[:, target] = model.predict_proba(X_targets)[:, 1]\n",
    "\n",
    "print(Y_test_pred.shape)\n",
    "display(Y_test_pred)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
