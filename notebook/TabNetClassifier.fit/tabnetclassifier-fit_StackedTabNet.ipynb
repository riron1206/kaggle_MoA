{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T07:51:44.591579Z",
     "start_time": "2020-11-18T07:51:44.588587Z"
    },
    "execution": {
     "iopub.execute_input": "2020-11-11T05:04:50.542867Z",
     "iopub.status.busy": "2020-11-11T05:04:50.541971Z",
     "iopub.status.idle": "2020-11-11T05:04:50.544528Z",
     "shell.execute_reply": "2020-11-11T05:04:50.545140Z"
    },
    "papermill": {
     "duration": 0.021419,
     "end_time": "2020-11-11T05:04:50.545288",
     "exception": false,
     "start_time": "2020-11-11T05:04:50.523869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "#sys.path.append(\"../input/adabeliefoptimizer/pypi_packages/adabelief_tf0.1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T07:51:44.595569Z",
     "start_time": "2020-11-18T07:51:44.592577Z"
    },
    "execution": {
     "iopub.execute_input": "2020-11-11T05:04:50.576328Z",
     "iopub.status.busy": "2020-11-11T05:04:50.575405Z",
     "iopub.status.idle": "2020-11-11T05:04:50.578063Z",
     "shell.execute_reply": "2020-11-11T05:04:50.578716Z"
    },
    "papermill": {
     "duration": 0.020088,
     "end_time": "2020-11-11T05:04:50.578862",
     "exception": false,
     "start_time": "2020-11-11T05:04:50.558774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "#sys.path.append(\"../input/iterative-stratification/iterative-stratification-master\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T07:51:44.599558Z",
     "start_time": "2020-11-18T07:51:44.596566Z"
    }
   },
   "outputs": [],
   "source": [
    "## GPU使わない\n",
    "#import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T07:51:46.524412Z",
     "start_time": "2020-11-18T07:51:44.600555Z"
    },
    "execution": {
     "iopub.execute_input": "2020-11-11T05:04:50.610018Z",
     "iopub.status.busy": "2020-11-11T05:04:50.609140Z",
     "iopub.status.idle": "2020-11-11T05:04:50.611546Z",
     "shell.execute_reply": "2020-11-11T05:04:50.612105Z"
    },
    "papermill": {
     "duration": 0.020067,
     "end_time": "2020-11-11T05:04:50.612217",
     "exception": false,
     "start_time": "2020-11-11T05:04:50.592150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.3.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "#sys.path.append(\"../input/tabnet\")\n",
    "\n",
    "sys.path.append(r'C:\\Users\\81908\\jupyter_notebook\\poetry_work\\tfgpu\\01_MoA_compe\\code')\n",
    "from tabnet_tf import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T07:51:46.532391Z",
     "start_time": "2020-11-18T07:51:46.525410Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-11-11T05:04:50.643552Z",
     "iopub.status.busy": "2020-11-11T05:04:50.642821Z",
     "iopub.status.idle": "2020-11-11T05:04:55.688316Z",
     "shell.execute_reply": "2020-11-11T05:04:55.687474Z"
    },
    "papermill": {
     "duration": 5.062476,
     "end_time": "2020-11-11T05:04:55.688467",
     "exception": false,
     "start_time": "2020-11-11T05:04:50.625991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from adabelief_tf import AdaBeliefOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T07:51:46.538375Z",
     "start_time": "2020-11-18T07:51:46.533388Z"
    },
    "code_folding": [
     3,
     5
    ],
    "execution": {
     "iopub.execute_input": "2020-11-11T05:04:55.746162Z",
     "iopub.status.busy": "2020-11-11T05:04:55.742354Z",
     "iopub.status.idle": "2020-11-11T05:04:55.749335Z",
     "shell.execute_reply": "2020-11-11T05:04:55.750123Z"
    },
    "papermill": {
     "duration": 0.039487,
     "end_time": "2020-11-11T05:04:55.750310",
     "exception": false,
     "start_time": "2020-11-11T05:04:55.710823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def build_callbacks(\n",
    "    model_path, factor=0.1, mode=\"auto\", monitor=\"val_loss\", patience=0, verbose=0\n",
    "):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        mode=mode, monitor=monitor, patience=patience, verbose=verbose\n",
    "    )\n",
    "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        model_path, mode=mode, monitor=monitor, save_best_only=True, verbose=verbose\n",
    "    )\n",
    "    reduce_lr_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        factor=factor, monitor=monitor, mode=mode, verbose=verbose\n",
    "    )\n",
    "\n",
    "    return [early_stopping, model_checkpoint, reduce_lr_on_plateau]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T07:51:46.744823Z",
     "start_time": "2020-11-18T07:51:46.540369Z"
    },
    "code_folding": [
     3
    ],
    "execution": {
     "iopub.execute_input": "2020-11-11T05:04:55.801970Z",
     "iopub.status.busy": "2020-11-11T05:04:55.801117Z",
     "iopub.status.idle": "2020-11-11T05:04:56.643869Z",
     "shell.execute_reply": "2020-11-11T05:04:56.642625Z"
    },
    "papermill": {
     "duration": 0.872492,
     "end_time": "2020-11-11T05:04:56.644014",
     "exception": false,
     "start_time": "2020-11-11T05:04:55.771522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "def score(Y, Y_pred):\n",
    "    _, n_classes = Y.shape\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for j in range(n_classes):\n",
    "        loss = log_loss(Y.iloc[:, j], Y_pred.iloc[:, j], labels=[0, 1])\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T07:51:46.749809Z",
     "start_time": "2020-11-18T07:51:46.745820Z"
    },
    "code_folding": [
     7
    ],
    "execution": {
     "iopub.execute_input": "2020-11-11T05:04:56.685510Z",
     "iopub.status.busy": "2020-11-11T05:04:56.684812Z",
     "iopub.status.idle": "2020-11-11T05:04:56.688206Z",
     "shell.execute_reply": "2020-11-11T05:04:56.687725Z"
    },
    "papermill": {
     "duration": 0.027592,
     "end_time": "2020-11-11T05:04:56.688308",
     "exception": false,
     "start_time": "2020-11-11T05:04:56.660716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random as rn\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "    rn.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    graph = tf.compat.v1.get_default_graph()\n",
    "    session_conf = tf.compat.v1.ConfigProto(\n",
    "        inter_op_parallelism_threads=1, intra_op_parallelism_threads=1\n",
    "    )\n",
    "    sess = tf.compat.v1.Session(graph=graph, config=session_conf)\n",
    "\n",
    "    tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T07:51:46.755793Z",
     "start_time": "2020-11-18T07:51:46.750807Z"
    },
    "code_folding": [
     4
    ],
    "execution": {
     "iopub.execute_input": "2020-11-11T05:04:56.727091Z",
     "iopub.status.busy": "2020-11-11T05:04:56.724972Z",
     "iopub.status.idle": "2020-11-11T05:04:56.729720Z",
     "shell.execute_reply": "2020-11-11T05:04:56.729176Z"
    },
    "papermill": {
     "duration": 0.026796,
     "end_time": "2020-11-11T05:04:56.729828",
     "exception": false,
     "start_time": "2020-11-11T05:04:56.703032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "\n",
    "class ClippedFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, copy=True, high=0.99, low=0.01):\n",
    "        self.copy = copy\n",
    "        self.high = high\n",
    "        self.low = low\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.data_max_ = X.quantile(q=self.high)\n",
    "        self.data_min_ = X.quantile(q=self.low)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.copy:\n",
    "            X = X.copy()\n",
    "\n",
    "        X.clip(self.data_min_, self.data_max_, axis=1, inplace=True)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T07:51:46.763772Z",
     "start_time": "2020-11-18T07:51:46.756791Z"
    },
    "code_folding": [
     6
    ],
    "execution": {
     "iopub.execute_input": "2020-11-11T05:04:56.776860Z",
     "iopub.status.busy": "2020-11-11T05:04:56.775099Z",
     "iopub.status.idle": "2020-11-11T05:04:56.777609Z",
     "shell.execute_reply": "2020-11-11T05:04:56.778140Z"
    },
    "papermill": {
     "duration": 0.033708,
     "end_time": "2020-11-11T05:04:56.778253",
     "exception": false,
     "start_time": "2020-11-11T05:04:56.744545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://arxiv.org/abs/1905.04899\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class Cutmix(tf.keras.utils.Sequence):\n",
    "    def __init__(self, X, y=None, batch_size=32, alpha=1.0):\n",
    "        self.X = np.asarray(X)\n",
    "\n",
    "        if y is None:\n",
    "            self.y = y\n",
    "        else:\n",
    "            self.y = np.asarray(y)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        X_batch = self.X[i * self.batch_size : (i + 1) * self.batch_size]\n",
    "\n",
    "        n_samples, n_features = self.X.shape\n",
    "        batch_size = X_batch.shape[0]\n",
    "        shuffle = np.random.choice(n_samples, batch_size)\n",
    "\n",
    "        l = np.random.beta(self.alpha, self.alpha)\n",
    "        mask = np.random.choice([0.0, 1.0], size=n_features, p=[1.0 - l, l])\n",
    "        X_shuffle = self.X[shuffle]\n",
    "        X_batch = mask * X_batch + (1.0 - mask) * X_shuffle\n",
    "\n",
    "        if self.y is None:\n",
    "            return X_batch, None\n",
    "\n",
    "        y_batch = self.y[i * self.batch_size : (i + 1) * self.batch_size]\n",
    "        y_shuffle = self.y[shuffle]\n",
    "        y_batch = l * y_batch + (1.0 - l) * y_shuffle\n",
    "\n",
    "        return X_batch, y_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        n_samples = self.X.shape[0]\n",
    "\n",
    "        return int(np.ceil(n_samples / self.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T07:51:46.777735Z",
     "start_time": "2020-11-18T07:51:46.764770Z"
    },
    "code_folding": [
     5
    ],
    "execution": {
     "iopub.execute_input": "2020-11-11T05:04:56.827302Z",
     "iopub.status.busy": "2020-11-11T05:04:56.826381Z",
     "iopub.status.idle": "2020-11-11T05:04:56.850273Z",
     "shell.execute_reply": "2020-11-11T05:04:56.849748Z"
    },
    "papermill": {
     "duration": 0.057135,
     "end_time": "2020-11-11T05:04:56.850370",
     "exception": false,
     "start_time": "2020-11-11T05:04:56.793235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.model_selection._split import _BaseKFold\n",
    "\n",
    "\n",
    "class MultilabelStratifiedGroupKFold(_BaseKFold):\n",
    "    def __init__(self, n_splits=5, random_state=None, shuffle=False):\n",
    "        super().__init__(n_splits=n_splits, random_state=random_state, shuffle=shuffle)\n",
    "\n",
    "    def _iter_test_indices(self, X=None, y=None, groups=None):\n",
    "        cv = MultilabelStratifiedKFold(\n",
    "            n_splits=self.n_splits,\n",
    "            random_state=self.random_state,\n",
    "            shuffle=self.shuffle,\n",
    "        )\n",
    "\n",
    "        value_counts = groups.value_counts()\n",
    "        regluar_indices = value_counts.loc[\n",
    "            (value_counts == 6) | (value_counts == 12) | (value_counts == 18)\n",
    "        ].index.sort_values()\n",
    "        irregluar_indices = value_counts.loc[\n",
    "            (value_counts != 6) & (value_counts != 12) & (value_counts != 18)\n",
    "        ].index.sort_values()\n",
    "\n",
    "        group_to_fold = {}\n",
    "        tmp = Y.groupby(groups).mean().loc[regluar_indices]\n",
    "\n",
    "        for fold, (_, test) in enumerate(cv.split(tmp, tmp)):\n",
    "            group_to_fold.update({group: fold for group in tmp.index[test]})\n",
    "\n",
    "        sample_to_fold = {}\n",
    "        tmp = Y.loc[groups.isin(irregluar_indices)]\n",
    "\n",
    "        for fold, (_, test) in enumerate(cv.split(tmp, tmp)):\n",
    "            sample_to_fold.update({sample: fold for sample in tmp.index[test]})\n",
    "\n",
    "        folds = groups.map(group_to_fold)\n",
    "        is_na = folds.isna()\n",
    "        folds[is_na] = folds[is_na].index.map(sample_to_fold).values\n",
    "\n",
    "        for i in range(self.n_splits):\n",
    "            yield np.where(folds == i)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T07:51:46.783718Z",
     "start_time": "2020-11-18T07:51:46.778732Z"
    },
    "code_folding": [
     4
    ],
    "execution": {
     "iopub.execute_input": "2020-11-11T05:04:56.886501Z",
     "iopub.status.busy": "2020-11-11T05:04:56.885537Z",
     "iopub.status.idle": "2020-11-11T05:04:56.890888Z",
     "shell.execute_reply": "2020-11-11T05:04:56.890343Z"
    },
    "incorrectly_encoded_metadata": "_kg_hide-input=true",
    "papermill": {
     "duration": 0.025247,
     "end_time": "2020-11-11T05:04:56.890989",
     "exception": false,
     "start_time": "2020-11-11T05:04:56.865742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from tabnet import StackedTabNet\n",
    "\n",
    "\n",
    "class StackedTabNetClassifier(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        batch_momentum=0.98,\n",
    "        epsilon=1e-05,\n",
    "        feature_columns=None,\n",
    "        feature_dim=64,\n",
    "        norm_type=\"group\",\n",
    "        num_decision_steps=5,\n",
    "        num_features=None,\n",
    "        num_groups=2,\n",
    "        num_layers=1,\n",
    "        output_dim=64,\n",
    "        relaxation_factor=1.5,\n",
    "        sparsity_coefficient=1e-05,\n",
    "        virtual_batch_size=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.stacked_tabnet = StackedTabNet(\n",
    "            feature_columns,\n",
    "            batch_momentum=batch_momentum,\n",
    "            epsilon=epsilon,\n",
    "            feature_dim=feature_dim,\n",
    "            norm_type=norm_type,\n",
    "            num_decision_steps=num_decision_steps,\n",
    "            num_features=num_features,\n",
    "            num_groups=num_groups,\n",
    "            num_layers=num_layers,\n",
    "            output_dim=output_dim,\n",
    "            relaxation_factor=relaxation_factor,\n",
    "            sparsity_coefficient=sparsity_coefficient,\n",
    "            virtual_batch_size=virtual_batch_size,\n",
    "        )\n",
    "\n",
    "        self.classifier = tf.keras.layers.Dense(\n",
    "            num_classes, activation=\"sigmoid\", use_bias=False\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.stacked_tabnet(inputs, training=training)\n",
    "\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T07:51:46.787708Z",
     "start_time": "2020-11-18T07:51:46.784716Z"
    },
    "execution": {
     "iopub.execute_input": "2020-11-11T05:04:56.991897Z",
     "iopub.status.busy": "2020-11-11T05:04:56.990903Z",
     "iopub.status.idle": "2020-11-11T05:05:02.545706Z",
     "shell.execute_reply": "2020-11-11T05:05:02.544763Z"
    },
    "papermill": {
     "duration": 5.578078,
     "end_time": "2020-11-11T05:05:02.545852",
     "exception": false,
     "start_time": "2020-11-11T05:04:56.967774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dtype = {\"cp_type\": \"category\", \"cp_dose\": \"category\"}\n",
    "#index_col = \"sig_id\"\n",
    "#\n",
    "#train_features = pd.read_csv(\n",
    "#    \"../input/lish-moa/train_features.csv\", dtype=dtype, index_col=index_col\n",
    "#)\n",
    "#X = train_features.select_dtypes(\"number\")\n",
    "#Y_nonscored = pd.read_csv(\n",
    "#    \"../input/lish-moa/train_targets_nonscored.csv\", index_col=index_col\n",
    "#)\n",
    "#Y = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\", index_col=index_col)\n",
    "#groups = pd.read_csv(\n",
    "#    \"../input/lish-moa/train_drug.csv\", index_col=index_col, squeeze=True\n",
    "#)\n",
    "#\n",
    "#columns = Y.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T07:51:50.606602Z",
     "start_time": "2020-11-18T07:51:46.788706Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    if \"jupyter_notebook\" in os.getcwd():\n",
    "        # load\n",
    "        dtype = {\"cp_type\": \"category\", \"cp_dose\": \"category\"}\n",
    "        index_col = \"sig_id\"\n",
    "\n",
    "        sys.path.append(\n",
    "            r\"C:\\Users\\81908\\jupyter_notebook\\poetry_work\\tfgpu\\01_MoA_compe\\code\"\n",
    "        )\n",
    "        import datasets\n",
    "\n",
    "        DATADIR = datasets.DATADIR\n",
    "\n",
    "        groups = pd.read_csv(\n",
    "            f\"{DATADIR}/train_drug.csv\", dtype=dtype, index_col=index_col, squeeze=True\n",
    "        )\n",
    "        train_features = pd.read_csv(\n",
    "            f\"{DATADIR}/train_features.csv\", dtype=dtype, index_col=index_col\n",
    "        )\n",
    "        X_test = pd.read_csv(\n",
    "            f\"{DATADIR}/test_features.csv\", dtype=dtype, index_col=index_col\n",
    "        )\n",
    "        X = train_features.select_dtypes(\"number\")\n",
    "        Y_nonscored = pd.read_csv(\n",
    "            f\"{DATADIR}/train_targets_nonscored.csv\", index_col=index_col\n",
    "        )\n",
    "        Y = pd.read_csv(f\"{DATADIR}/train_targets_scored.csv\", index_col=index_col)\n",
    "\n",
    "        columns = Y.columns\n",
    "\n",
    "    else:\n",
    "        # load\n",
    "        dtype = {\"cp_type\": \"category\", \"cp_dose\": \"category\"}\n",
    "        index_col = \"sig_id\"\n",
    "\n",
    "        groups = pd.read_csv(\n",
    "            f\"../input/lish-moa/train_drug.csv\",\n",
    "            dtype=dtype,\n",
    "            index_col=index_col,\n",
    "            squeeze=True,\n",
    "        )\n",
    "        train_features = pd.read_csv(\n",
    "            \"../input/lish-moa/train_features.csv\", dtype=dtype, index_col=index_col\n",
    "        )\n",
    "        X_test = pd.read_csv(\n",
    "            \"../input/lish-moa/test_features.csv\", dtype=dtype, index_col=index_col\n",
    "        )\n",
    "        X = train_features.select_dtypes(\"number\")\n",
    "        Y_nonscored = pd.read_csv(\n",
    "            \"../input/lish-moa/train_targets_nonscored.csv\", index_col=index_col\n",
    "        )\n",
    "        Y = pd.read_csv(\n",
    "            \"../input/lish-moa/train_targets_scored.csv\", index_col=index_col\n",
    "        )\n",
    "\n",
    "        columns = Y.columns\n",
    "\n",
    "    return X, Y, Y_nonscored, train_features, columns, groups, X_test\n",
    "\n",
    "X, Y, Y_nonscored, train_features, columns, groups, X_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T07:51:52.556389Z",
     "start_time": "2020-11-18T07:51:50.607600Z"
    },
    "execution": {
     "iopub.execute_input": "2020-11-11T05:05:02.584650Z",
     "iopub.status.busy": "2020-11-11T05:05:02.583768Z",
     "iopub.status.idle": "2020-11-11T05:05:03.740853Z",
     "shell.execute_reply": "2020-11-11T05:05:03.739501Z"
    },
    "papermill": {
     "duration": 1.178911,
     "end_time": "2020-11-11T05:05:03.741004",
     "exception": false,
     "start_time": "2020-11-11T05:05:02.562093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#with open(\"../input/mlpclassifierfit/clipped_features.pkl\", \"rb\") as f:\n",
    "#    clipped_features = pickle.load(f)\n",
    "#X = clipped_features.transform(X)\n",
    "clipped_features = ClippedFeatures()\n",
    "X = clipped_features.fit_transform(X)\n",
    "\n",
    "with open(\"clipped_features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(clipped_features, f)\n",
    "\n",
    "#Y_nonscored = Y_nonscored.loc[:, Y_nonscored.sum(axis=0) > 0]\n",
    "#Y = pd.concat([Y, Y_nonscored], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T07:51:52.560379Z",
     "start_time": "2020-11-18T07:51:52.557387Z"
    },
    "execution": {
     "iopub.execute_input": "2020-11-11T05:05:03.780131Z",
     "iopub.status.busy": "2020-11-11T05:05:03.778057Z",
     "iopub.status.idle": "2020-11-11T05:05:03.780921Z",
     "shell.execute_reply": "2020-11-11T05:05:03.781504Z"
    },
    "papermill": {
     "duration": 0.024865,
     "end_time": "2020-11-11T05:05:03.781629",
     "exception": false,
     "start_time": "2020-11-11T05:05:03.756764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_size, n_features = X.shape\n",
    "# _, n_classes_nonscored = Y_nonscored.shape\n",
    "_, n_classes = Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T07:51:52.565366Z",
     "start_time": "2020-11-18T07:51:52.561377Z"
    },
    "execution": {
     "iopub.execute_input": "2020-11-11T05:05:03.823509Z",
     "iopub.status.busy": "2020-11-11T05:05:03.822789Z",
     "iopub.status.idle": "2020-11-11T05:05:03.826609Z",
     "shell.execute_reply": "2020-11-11T05:05:03.825995Z"
    },
    "papermill": {
     "duration": 0.027961,
     "end_time": "2020-11-11T05:05:03.826751",
     "exception": false,
     "start_time": "2020-11-11T05:05:03.798790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 8\n",
    "factor = 0.5\n",
    "label_smoothing = 1e-03\n",
    "lr = 0.001\n",
    "n_seeds = 5\n",
    "n_splits = 5\n",
    "patience = 30\n",
    "shuffle = True\n",
    "params = {\n",
    "    \"batch_momentum\": 0.95,\n",
    "    \"feature_dim\": 512,\n",
    "    \"norm_type\": \"batch\",\n",
    "    \"num_decision_steps\": 1,\n",
    "    \"num_layers\": 2,\n",
    "}\n",
    "fit_params = {\"epochs\": 1_000, \"verbose\": 0}\n",
    "#fit_params = {\"epochs\": 80, \"verbose\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-18T07:51:44.605Z"
    },
    "execution": {
     "iopub.execute_input": "2020-11-11T05:05:03.869074Z",
     "iopub.status.busy": "2020-11-11T05:05:03.868261Z",
     "iopub.status.idle": "2020-11-11T10:55:44.832963Z",
     "shell.execute_reply": "2020-11-11T10:55:44.833547Z"
    },
    "papermill": {
     "duration": 21040.991001,
     "end_time": "2020-11-11T10:55:44.833801",
     "exception": false,
     "start_time": "2020-11-11T05:05:03.842800",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:From <timed exec>:28: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass shuffle=True, random_state=1 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass shuffle=True, random_state=2 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass shuffle=True, random_state=3 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Y_pred = np.zeros((train_size, n_classes))\n",
    "Y_pred = pd.DataFrame(Y_pred, columns=Y.columns, index=Y.index)\n",
    "\n",
    "for i in range(n_seeds):\n",
    "    set_seed(seed=i)\n",
    "\n",
    "    cv = MultilabelStratifiedGroupKFold(\n",
    "        n_splits=n_splits, random_state=i, shuffle=shuffle\n",
    "    )\n",
    "\n",
    "    for j, (train, valid) in enumerate(cv.split(X, Y, groups)):\n",
    "        model_path = f\"model_seed_{i}_fold_{j}.h5\"\n",
    "\n",
    "        K.clear_session()\n",
    "        model = StackedTabNetClassifier(\n",
    "                num_classes=n_classes, num_features=n_features, **params\n",
    "        )\n",
    "        loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing)\n",
    "        optimizer = AdaBeliefOptimizer(learning_rate=lr)\n",
    "\n",
    "        model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "        generator = Cutmix(X.iloc[train], Y.iloc[train], batch_size=batch_size)\n",
    "        callbacks = build_callbacks(model_path, factor=factor, patience=patience)\n",
    "        history = model.fit_generator(\n",
    "            generator,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=(X.iloc[valid], Y.iloc[valid]),\n",
    "            **fit_params,\n",
    "        )\n",
    "\n",
    "        model.load_weights(model_path)\n",
    "\n",
    "        Y_pred.iloc[valid] += model.predict(X.iloc[valid]) / n_seeds\n",
    "\n",
    "Y_pred[train_features[\"cp_type\"] == \"ctl_vehicle\"] = 0.0\n",
    "\n",
    "with open(\"Y_pred.pkl\", \"wb\") as f:\n",
    "    pickle.dump(Y_pred[columns], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-18T07:51:44.606Z"
    },
    "execution": {
     "iopub.execute_input": "2020-11-11T10:55:44.916464Z",
     "iopub.status.busy": "2020-11-11T10:55:44.915550Z",
     "iopub.status.idle": "2020-11-11T10:55:45.839789Z",
     "shell.execute_reply": "2020-11-11T10:55:45.840343Z"
    },
    "papermill": {
     "duration": 0.978402,
     "end_time": "2020-11-11T10:55:45.840496",
     "exception": false,
     "start_time": "2020-11-11T10:55:44.862094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "score(Y[columns], Y_pred[columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-18T07:51:44.608Z"
    }
   },
   "outputs": [],
   "source": [
    "model = StackedTabNetClassifier(num_classes=n_classes, num_features=n_features, **params)\n",
    "model(np.zeros((1, n_features)))\n",
    "model.load_weights(f\"./model_seed_0_fold_0.h5\")\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.6.0"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "papermill": {
   "duration": 21060.955873,
   "end_time": "2020-11-11T10:55:47.598439",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-11T05:04:46.642566",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
