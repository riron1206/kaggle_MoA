{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考: https://www.kaggle.com/yxohrxn/mlpclassifier-fit?scriptVersionId=46905918"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T07:49:28.923883Z",
     "start_time": "2020-11-29T07:49:28.920891Z"
    },
    "execution": {
     "iopub.execute_input": "2020-11-15T04:40:11.867203Z",
     "iopub.status.busy": "2020-11-15T04:40:11.866191Z",
     "iopub.status.idle": "2020-11-15T04:40:11.869103Z",
     "shell.execute_reply": "2020-11-15T04:40:11.868399Z"
    },
    "papermill": {
     "duration": 0.029988,
     "end_time": "2020-11-15T04:40:11.869223",
     "exception": false,
     "start_time": "2020-11-15T04:40:11.839235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# sys.path.append(\"../input/adabeliefoptimizer/pypi_packages/adabelief_tf0.1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T07:49:29.747578Z",
     "start_time": "2020-11-29T07:49:29.744566Z"
    },
    "execution": {
     "iopub.execute_input": "2020-11-15T04:40:11.916751Z",
     "iopub.status.busy": "2020-11-15T04:40:11.915736Z",
     "iopub.status.idle": "2020-11-15T04:40:11.919098Z",
     "shell.execute_reply": "2020-11-15T04:40:11.918390Z"
    },
    "papermill": {
     "duration": 0.028727,
     "end_time": "2020-11-15T04:40:11.919213",
     "exception": false,
     "start_time": "2020-11-15T04:40:11.890486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# sys.path.append(\"../input/iterative-stratification/iterative-stratification-master\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T07:49:30.482774Z",
     "start_time": "2020-11-29T07:49:30.479781Z"
    }
   },
   "outputs": [],
   "source": [
    "# GPUメモリ必要な分だけ確保\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T07:49:31.210871Z",
     "start_time": "2020-11-29T07:49:31.208827Z"
    }
   },
   "outputs": [],
   "source": [
    "# # GPU使わない\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T07:49:35.317841Z",
     "start_time": "2020-11-29T07:49:31.954606Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-11-15T04:40:11.967087Z",
     "iopub.status.busy": "2020-11-15T04:40:11.966155Z",
     "iopub.status.idle": "2020-11-15T04:40:17.400927Z",
     "shell.execute_reply": "2020-11-15T04:40:17.400141Z"
    },
    "papermill": {
     "duration": 5.46079,
     "end_time": "2020-11-15T04:40:17.401049",
     "exception": false,
     "start_time": "2020-11-15T04:40:11.940259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T07:49:35.321831Z",
     "start_time": "2020-11-29T07:49:35.318838Z"
    },
    "code_folding": [],
    "execution": {
     "iopub.execute_input": "2020-11-15T04:40:17.455544Z",
     "iopub.status.busy": "2020-11-15T04:40:17.454466Z",
     "iopub.status.idle": "2020-11-15T04:40:17.457712Z",
     "shell.execute_reply": "2020-11-15T04:40:17.457108Z"
    },
    "papermill": {
     "duration": 0.034006,
     "end_time": "2020-11-15T04:40:17.457833",
     "exception": false,
     "start_time": "2020-11-15T04:40:17.423827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def build_callbacks(\n",
    "    model_path, factor=0.1, mode=\"auto\", monitor=\"val_loss\", patience=0, verbose=0\n",
    "):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        mode=mode, monitor=monitor, patience=patience, verbose=verbose\n",
    "    )\n",
    "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        model_path, mode=mode, monitor=monitor, save_best_only=True, verbose=verbose\n",
    "    )\n",
    "    reduce_lr_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        factor=factor, monitor=monitor, mode=mode, verbose=verbose\n",
    "    )\n",
    "\n",
    "    return [early_stopping, model_checkpoint, reduce_lr_on_plateau]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T07:49:35.452443Z",
     "start_time": "2020-11-29T07:49:35.322802Z"
    },
    "code_folding": [
     13,
     96,
     163,
     219,
     265
    ]
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from adabelief_tf import AdaBeliefOptimizer\n",
    "\n",
    "\n",
    "def create_model_rs(\n",
    "    shape1,\n",
    "    shape2,\n",
    "    num_classes=206,\n",
    "    lr=0.03,\n",
    "    activations=[\"selu\", \"selu\", \"selu\", \"selu\", \"selu\", \"selu\", \"relu\", \"selu\"],\n",
    "    denses=[369, 939, 696, 970, 887, 991],\n",
    "    drop_rates=[0.777298030070047, 0.6033124455518569],\n",
    "):\n",
    "    \"\"\"入力2つのNN.resnetみたくskip connection入れてる\"\"\"\n",
    "    input_1 = tf.keras.layers.Input(shape=(shape1))\n",
    "    input_2 = tf.keras.layers.Input(shape=(shape2))\n",
    "\n",
    "    head_1 = tf.keras.layers.BatchNormalization()(input_1)\n",
    "    head_1 = tf.keras.layers.Dropout(drop_rates[0])(head_1)\n",
    "    head_1 = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[0], activation=activations[0])\n",
    "    )(head_1)\n",
    "    head_1 = tf.keras.layers.BatchNormalization()(head_1)\n",
    "    input_3 = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[1], activation=activations[1])\n",
    "    )(head_1)\n",
    "\n",
    "    input_3_concat = tf.keras.layers.Concatenate()(\n",
    "        [input_2, input_3]\n",
    "    )  # node連結。node数が2つのnodeの足し算になる\n",
    "\n",
    "    head_2 = tf.keras.layers.BatchNormalization()(input_3_concat)\n",
    "    head_2 = tf.keras.layers.Dropout(drop_rates[1])(head_2)\n",
    "    head_2 = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[2], activations[2])\n",
    "    )(head_2)\n",
    "    head_2 = tf.keras.layers.BatchNormalization()(head_2)\n",
    "    head_2 = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[3], activations[3])\n",
    "    )(head_2)\n",
    "    head_2 = tf.keras.layers.BatchNormalization()(head_2)\n",
    "    head_2 = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[4], activations[4])\n",
    "    )(head_2)\n",
    "    head_2 = tf.keras.layers.BatchNormalization()(head_2)\n",
    "    input_4 = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[1], activations[5])\n",
    "    )(\n",
    "        head_2\n",
    "    )  # input_3 と同じnode数でないとだめ. tf.keras.layers.Average するから\n",
    "\n",
    "    input_4_avg = tf.keras.layers.Average()([input_3, input_4])  # 入力のリストを要素ごとに平均化\n",
    "\n",
    "    head_3 = tf.keras.layers.BatchNormalization()(input_4_avg)\n",
    "    head_3 = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(\n",
    "            denses[5], kernel_initializer=\"lecun_normal\", activation=activations[6]\n",
    "        )\n",
    "    )(head_3)\n",
    "    head_3 = tf.keras.layers.BatchNormalization()(head_3)\n",
    "    head_3 = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(\n",
    "            num_classes, kernel_initializer=\"lecun_normal\", activation=activations[7]\n",
    "        )\n",
    "    )(head_3)\n",
    "    head_3 = tf.keras.layers.BatchNormalization()(head_3)\n",
    "    output = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(num_classes, activation=\"sigmoid\")\n",
    "    )(head_3)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[input_1, input_2], outputs=output)\n",
    "    opt = AdaBeliefOptimizer(learning_rate=lr)\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.0015),  # ラベルスムージング\n",
    "        metrics=tf.keras.metrics.BinaryCrossentropy(),\n",
    "    )\n",
    "    # model.save(\"model/rs_shape.h5\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_5l(\n",
    "    shape,\n",
    "    num_classes=206,\n",
    "    lr=0.03,\n",
    "    activation=\"relu\",\n",
    "    denses=[512, 512, 512, 512, 512],\n",
    "    drop_rates=[\n",
    "        0.28593716228565935,\n",
    "        0.3271533962453585,\n",
    "        0.5271338848921763,\n",
    "        0.2676199676497088,\n",
    "        0.5363898352447071,\n",
    "        0.4021564764052554,\n",
    "    ],\n",
    "):\n",
    "    \"\"\"入力1つの5層NN。Stochastic Weight Averaging使う\"\"\"\n",
    "    inp = tf.keras.layers.Input(shape=(shape))\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(inp)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[0])(x)\n",
    "    x = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[0], activation=activation)\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[1])(x)\n",
    "    x = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[1], activation=activation)\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[2])(x)\n",
    "    x = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[2], activation=activation)\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[3])(x)\n",
    "    x = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[3], activation=activation)\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[4])(x)\n",
    "    x = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[4], activation=activation)\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[5])(x)\n",
    "    out = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(num_classes, activation=\"sigmoid\")\n",
    "    )(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=out)\n",
    "    opt = tf.optimizers.Adam(learning_rate=lr)\n",
    "    opt = tfa.optimizers.SWA(\n",
    "        opt\n",
    "    )  # Stochastic Weight Averaging.モデルの重みを、これまで＋今回の平均を取って更新していくことでうまくいくみたい https://twitter.com/icoxfog417/status/989762534163992577\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.0020),  # ラベルスムージング\n",
    "        metrics=tf.keras.metrics.BinaryCrossentropy(),\n",
    "    )\n",
    "    # model.save(\"model/5l_shape.h5\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_4l(\n",
    "    shape,\n",
    "    num_classes=206,\n",
    "    lr=0.03,\n",
    "    activation=\"relu\",\n",
    "    denses=[512, 448, 384, 320],\n",
    "    drop_rates=[\n",
    "        0.20043759292966096,\n",
    "        0.25143314190089106,\n",
    "        0.4898856244488173,\n",
    "        0.25296015385762904,\n",
    "        0.338214056622176,\n",
    "    ],\n",
    "):\n",
    "    \"\"\"入力1つの4層NN。シンプル\"\"\"\n",
    "    inp = tf.keras.layers.Input(shape=(shape))\n",
    "    x = tf.keras.layers.BatchNormalization()(inp)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[0])(x)\n",
    "    x = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[0], activation=activation)\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[1])(x)\n",
    "    x = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[1], activation=activation)\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[2])(x)\n",
    "    x = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[2], activation=activation)\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[3])(x)\n",
    "    x = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[3], activation=activation)\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[4])(x)\n",
    "    out = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(num_classes, activation=\"sigmoid\")\n",
    "    )(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=out)\n",
    "    opt = tf.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.0020),  # ラベルスムージング\n",
    "        metrics=tf.keras.metrics.BinaryCrossentropy(),\n",
    "    )\n",
    "    # model.save(\"model/4l_shape.h5\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_3l_v2(\n",
    "    shape,\n",
    "    num_classes=206,\n",
    "    lr=0.03,\n",
    "    activation=\"selu\",\n",
    "    denses=[1024, 1024, 1024],\n",
    "    drop_rates=[\n",
    "        0.12742511520132901,\n",
    "        0.5367078212868925,\n",
    "        0.3914765737655165,\n",
    "        0.20302740274780187,\n",
    "    ],\n",
    "):\n",
    "    \"\"\"入力1つの3層NN。adabelief_tf 使う\"\"\"\n",
    "    inp = tf.keras.layers.Input(shape=(shape))\n",
    "    x = tf.keras.layers.BatchNormalization()(inp)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[0])(x)\n",
    "    x = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[0], activation=activation)\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[1])(x)\n",
    "    x = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[1], activation=activation)\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[2])(x)\n",
    "    x = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[2], activation=activation)\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[3])(x)\n",
    "    out = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(num_classes, activation=\"sigmoid\")\n",
    "    )(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=out)\n",
    "    opt = AdaBeliefOptimizer(learning_rate=lr)\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.0015),\n",
    "        metrics=tf.keras.metrics.BinaryCrossentropy(),\n",
    "    )\n",
    "    # model.save(\"model/3l_shape.h5\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_2l(\n",
    "    shape,\n",
    "    num_classes=206,\n",
    "    lr=0.03,\n",
    "    activation=\"relu\",\n",
    "    denses=[1150, 1371],\n",
    "    drop_rates=[0.19506281094918945, 0.22193390554785974, 0.7161410458761702],\n",
    "    sync_period=38,\n",
    "):\n",
    "    \"\"\"入力1つの2層NN。Lookahead, WeightNormalization使う\"\"\"\n",
    "    inp = tf.keras.layers.Input(shape=(shape))\n",
    "    x = tf.keras.layers.BatchNormalization()(inp)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[0])(x)\n",
    "    x = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[0], activation=activation)\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[1])(x)\n",
    "    x = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[1], activation=activation)\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[2])(x)\n",
    "    out = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(num_classes, activation=\"sigmoid\")\n",
    "    )(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=out)\n",
    "    opt = tf.optimizers.Adam(learning_rate=lr)\n",
    "    opt = tfa.optimizers.Lookahead(opt, sync_period=sync_period)\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.0015),\n",
    "        metrics=tf.keras.metrics.BinaryCrossentropy(),\n",
    "    )\n",
    "    # model.save(\"model/2l_shape.h5\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T07:49:35.463178Z",
     "start_time": "2020-11-29T07:49:35.452757Z"
    },
    "code_folding": [
     11,
     29
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.3.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(r\"C:\\Users\\81908\\jupyter_notebook\\poetry_work\\tfgpu\\01_MoA_compe\\code\")\n",
    "from tabnet_tf import *\n",
    "\n",
    "# from tabnet import StackedTabNet\n",
    "\n",
    "import tensorflow as tf\n",
    "from adabelief_tf import AdaBeliefOptimizer\n",
    "\n",
    "\n",
    "class StackedTabNetClassifier(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        batch_momentum=0.98,\n",
    "        epsilon=1e-05,\n",
    "        feature_columns=None,\n",
    "        feature_dim=64,\n",
    "        norm_type=\"group\",\n",
    "        num_decision_steps=5,\n",
    "        num_features=None,\n",
    "        num_groups=2,\n",
    "        num_layers=1,\n",
    "        output_dim=64,\n",
    "        relaxation_factor=1.5,\n",
    "        sparsity_coefficient=1e-05,\n",
    "        virtual_batch_size=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.stacked_tabnet = StackedTabNet(\n",
    "            feature_columns,\n",
    "            batch_momentum=batch_momentum,\n",
    "            epsilon=epsilon,\n",
    "            feature_dim=feature_dim,\n",
    "            norm_type=norm_type,\n",
    "            num_decision_steps=num_decision_steps,\n",
    "            num_features=num_features,\n",
    "            num_groups=num_groups,\n",
    "            num_layers=num_layers,\n",
    "            output_dim=output_dim,\n",
    "            relaxation_factor=relaxation_factor,\n",
    "            sparsity_coefficient=sparsity_coefficient,\n",
    "            virtual_batch_size=virtual_batch_size,\n",
    "        )\n",
    "\n",
    "        self.classifier = tf.keras.layers.Dense(\n",
    "            num_classes, activation=\"sigmoid\", use_bias=False\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.stacked_tabnet(inputs, training=training)\n",
    "\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "def create_model_stacked_tabnet(\n",
    "    n_features, num_classes=206, lr=0.001,\n",
    "):\n",
    "    model = StackedTabNetClassifier(\n",
    "        num_classes=num_classes, num_features=n_features, **stacked_tabnet_params,\n",
    "    )\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=1e-03)\n",
    "    optimizer = AdaBeliefOptimizer(learning_rate=lr)\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T07:49:35.881468Z",
     "start_time": "2020-11-29T07:49:35.463178Z"
    },
    "code_folding": [
     3
    ],
    "execution": {
     "iopub.execute_input": "2020-11-15T04:40:18.021251Z",
     "iopub.status.busy": "2020-11-15T04:40:18.020509Z",
     "iopub.status.idle": "2020-11-15T04:40:18.794904Z",
     "shell.execute_reply": "2020-11-15T04:40:18.794072Z"
    },
    "papermill": {
     "duration": 0.805422,
     "end_time": "2020-11-15T04:40:18.795031",
     "exception": false,
     "start_time": "2020-11-15T04:40:17.989609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "def score(Y, Y_pred):\n",
    "    _, n_classes = Y.shape\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for j in range(n_classes):\n",
    "        loss = log_loss(Y.iloc[:, j], Y_pred.iloc[:, j], labels=[0, 1])\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T07:49:36.421577Z",
     "start_time": "2020-11-29T07:49:36.417608Z"
    },
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def auc_score(Y, Y_pred):\n",
    "    _, n_classes = Y.shape\n",
    "\n",
    "    aucs = []\n",
    "\n",
    "    for j in range(n_classes):\n",
    "        auc = roc_auc_score(Y.iloc[:, j], Y_pred.iloc[:, j])\n",
    "\n",
    "        aucs.append(auc)\n",
    "\n",
    "    return np.mean(aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T07:49:37.258573Z",
     "start_time": "2020-11-29T07:49:37.253608Z"
    },
    "code_folding": [
     7
    ],
    "execution": {
     "iopub.execute_input": "2020-11-15T04:40:18.852597Z",
     "iopub.status.busy": "2020-11-15T04:40:18.851571Z",
     "iopub.status.idle": "2020-11-15T04:40:18.855082Z",
     "shell.execute_reply": "2020-11-15T04:40:18.854311Z"
    },
    "papermill": {
     "duration": 0.036662,
     "end_time": "2020-11-15T04:40:18.855208",
     "exception": false,
     "start_time": "2020-11-15T04:40:18.818546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random as rn\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "    rn.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    graph = tf.compat.v1.get_default_graph()\n",
    "    session_conf = tf.compat.v1.ConfigProto(\n",
    "        inter_op_parallelism_threads=1, intra_op_parallelism_threads=1\n",
    "    )\n",
    "    sess = tf.compat.v1.Session(graph=graph, config=session_conf)\n",
    "\n",
    "    tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T07:49:38.057987Z",
     "start_time": "2020-11-29T07:49:38.053024Z"
    },
    "code_folding": [
     4
    ],
    "execution": {
     "iopub.execute_input": "2020-11-15T04:40:18.913724Z",
     "iopub.status.busy": "2020-11-15T04:40:18.912896Z",
     "iopub.status.idle": "2020-11-15T04:40:18.916149Z",
     "shell.execute_reply": "2020-11-15T04:40:18.915459Z"
    },
    "papermill": {
     "duration": 0.037276,
     "end_time": "2020-11-15T04:40:18.916269",
     "exception": false,
     "start_time": "2020-11-15T04:40:18.878993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "\n",
    "class ClippedFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, copy=True, high=0.99, low=0.01):\n",
    "        self.copy = copy\n",
    "        self.high = high\n",
    "        self.low = low\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.data_max_ = X.quantile(q=self.high)\n",
    "        self.data_min_ = X.quantile(q=self.low)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.copy:\n",
    "            X = X.copy()\n",
    "\n",
    "        X.clip(self.data_min_, self.data_max_, axis=1, inplace=True)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T07:49:38.966542Z",
     "start_time": "2020-11-29T07:49:38.959560Z"
    },
    "incorrectly_encoded_metadata": "code_folding=[6] code_folding=[6] execution={\"iopub.execute_input\": \"2020-11-15T04:40:18.982852Z\", \"iopub.status.busy\": \"2020-11-15T04:40:18.981785Z\", \"iopub.status.idle\": \"2020-11-15T04:40:18.985199Z\", \"shell.execute_reply\": \"2020-11-15T04:40:18.984617Z\"} _kg_hide-input=false",
    "papermill": {
     "duration": 0.045366,
     "end_time": "2020-11-15T04:40:18.985320",
     "exception": false,
     "start_time": "2020-11-15T04:40:18.939954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://arxiv.org/abs/1905.04899\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class Cutmix(tf.keras.utils.Sequence):\n",
    "    def __init__(self, X, y=None, batch_size=32, alpha=1.0):\n",
    "        self.X = np.asarray(X)\n",
    "\n",
    "        if y is None:\n",
    "            self.y = y\n",
    "        else:\n",
    "            self.y = np.asarray(y)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        X_batch = self.X[i * self.batch_size : (i + 1) * self.batch_size]\n",
    "\n",
    "        n_samples, n_features = self.X.shape\n",
    "        batch_size = X_batch.shape[0]\n",
    "        shuffle = np.random.choice(n_samples, batch_size)\n",
    "\n",
    "        l = np.random.beta(self.alpha, self.alpha)\n",
    "        mask = np.random.choice([0.0, 1.0], size=n_features, p=[1.0 - l, l])\n",
    "        X_shuffle = self.X[shuffle]\n",
    "        X_batch = mask * X_batch + (1.0 - mask) * X_shuffle\n",
    "\n",
    "        if self.y is None:\n",
    "            return X_batch, None\n",
    "\n",
    "        y_batch = self.y[i * self.batch_size : (i + 1) * self.batch_size]\n",
    "        y_shuffle = self.y[shuffle]\n",
    "        y_batch = l * y_batch + (1.0 - l) * y_shuffle\n",
    "\n",
    "        return X_batch, y_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        n_samples = self.X.shape[0]\n",
    "\n",
    "        return int(np.ceil(n_samples / self.batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T07:49:41.010440Z",
     "start_time": "2020-11-29T07:49:40.986509Z"
    },
    "code_folding": [
     5
    ],
    "execution": {
     "iopub.execute_input": "2020-11-15T04:40:19.223235Z",
     "iopub.status.busy": "2020-11-15T04:40:19.222417Z",
     "iopub.status.idle": "2020-11-15T04:40:19.248983Z",
     "shell.execute_reply": "2020-11-15T04:40:19.248301Z"
    },
    "papermill": {
     "duration": 0.069383,
     "end_time": "2020-11-15T04:40:19.249102",
     "exception": false,
     "start_time": "2020-11-15T04:40:19.179719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.model_selection._split import _BaseKFold\n",
    "\n",
    "\n",
    "class MultilabelStratifiedGroupKFold(_BaseKFold):\n",
    "    def __init__(self, n_splits=5, random_state=None, shuffle=False):\n",
    "        super().__init__(n_splits=n_splits, random_state=random_state, shuffle=shuffle)\n",
    "\n",
    "    def _iter_test_indices(self, X=None, Y=None, groups=None):\n",
    "        cv = MultilabelStratifiedKFold(\n",
    "            n_splits=self.n_splits,\n",
    "            random_state=self.random_state,\n",
    "            shuffle=self.shuffle,\n",
    "        )\n",
    "\n",
    "        value_counts = groups.value_counts()\n",
    "        regluar_indices = value_counts.loc[\n",
    "            (value_counts == 6) | (value_counts == 12) | (value_counts == 18)\n",
    "        ].index.sort_values()\n",
    "        irregluar_indices = value_counts.loc[\n",
    "            (value_counts != 6) & (value_counts != 12) & (value_counts != 18)\n",
    "        ].index.sort_values()\n",
    "\n",
    "        group_to_fold = {}\n",
    "        tmp = Y.groupby(groups).mean().loc[regluar_indices]\n",
    "\n",
    "        for fold, (_, test) in enumerate(cv.split(tmp, tmp)):\n",
    "            group_to_fold.update({group: fold for group in tmp.index[test]})\n",
    "\n",
    "        sample_to_fold = {}\n",
    "        tmp = Y.loc[groups.isin(irregluar_indices)]\n",
    "\n",
    "        for fold, (_, test) in enumerate(cv.split(tmp, tmp)):\n",
    "            sample_to_fold.update({sample: fold for sample in tmp.index[test]})\n",
    "\n",
    "        folds = groups.map(group_to_fold)\n",
    "        is_na = folds.isna()\n",
    "        folds[is_na] = folds[is_na].index.map(sample_to_fold).values\n",
    "\n",
    "        for i in range(self.n_splits):\n",
    "            yield np.where(folds == i)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T07:49:45.820753Z",
     "start_time": "2020-11-29T07:49:42.935652Z"
    }
   },
   "outputs": [],
   "source": [
    "dtype = {\"cp_type\": \"category\", \"cp_dose\": \"category\"}\n",
    "index_col = \"sig_id\"\n",
    "\n",
    "sys.path.append(r\"C:\\Users\\81908\\jupyter_notebook\\poetry_work\\tfgpu\\01_MoA_compe\\code\")\n",
    "import datasets\n",
    "\n",
    "DATADIR = datasets.DATADIR\n",
    "\n",
    "groups = pd.read_csv(\n",
    "    f\"{DATADIR}/train_drug.csv\", dtype=dtype, index_col=index_col, squeeze=True\n",
    ")\n",
    "train_features = pd.read_csv(\n",
    "    f\"{DATADIR}/train_features.csv\", dtype=dtype, index_col=index_col\n",
    ")\n",
    "# X_test = pd.read_csv(f\"{DATADIR}/test_features.csv\", dtype=dtype, index_col=index_col)\n",
    "X = train_features.select_dtypes(\"number\")\n",
    "Y_nonscored = pd.read_csv(f\"{DATADIR}/train_targets_nonscored.csv\", index_col=index_col)\n",
    "Y = pd.read_csv(f\"{DATADIR}/train_targets_scored.csv\", index_col=index_col)\n",
    "\n",
    "columns = Y.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T07:49:47.477359Z",
     "start_time": "2020-11-29T07:49:45.821751Z"
    },
    "execution": {
     "iopub.execute_input": "2020-11-15T04:40:25.034407Z",
     "iopub.status.busy": "2020-11-15T04:40:25.033563Z",
     "iopub.status.idle": "2020-11-15T04:40:27.251916Z",
     "shell.execute_reply": "2020-11-15T04:40:27.251250Z"
    },
    "papermill": {
     "duration": 2.253691,
     "end_time": "2020-11-15T04:40:27.252051",
     "exception": false,
     "start_time": "2020-11-15T04:40:24.998360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clipped_features = ClippedFeatures()\n",
    "X = clipped_features.fit_transform(X)\n",
    "\n",
    "with open(\"clipped_features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(clipped_features, f)\n",
    "\n",
    "# c_prefix = \"c-\"\n",
    "# g_prefix = \"g-\"\n",
    "# c_columns = X.columns.str.startswith(c_prefix)\n",
    "# g_columns = X.columns.str.startswith(g_prefix)\n",
    "# X_c = compute_row_statistics(X.loc[:, c_columns], prefix=c_prefix)\n",
    "# X_g = compute_row_statistics(X.loc[:, g_columns], prefix=g_prefix)\n",
    "# X = pd.concat([X, X_c, X_g], axis=1)\n",
    "\n",
    "# Y_nonscored = Y_nonscored.loc[:, Y_nonscored.sum(axis=0) > 0]\n",
    "# Y = pd.concat([Y, Y_nonscored], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T07:49:47.481349Z",
     "start_time": "2020-11-29T07:49:47.478357Z"
    },
    "execution": {
     "iopub.execute_input": "2020-11-15T04:40:27.307722Z",
     "iopub.status.busy": "2020-11-15T04:40:27.306775Z",
     "iopub.status.idle": "2020-11-15T04:40:27.309988Z",
     "shell.execute_reply": "2020-11-15T04:40:27.309231Z"
    },
    "papermill": {
     "duration": 0.033192,
     "end_time": "2020-11-15T04:40:27.310108",
     "exception": false,
     "start_time": "2020-11-15T04:40:27.276916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_classes: 206\n"
     ]
    }
   ],
   "source": [
    "train_size, n_features = X.shape\n",
    "_, n_classes_nonscored = Y_nonscored.shape\n",
    "_, n_classes = Y.shape\n",
    "print(f\"n_classes: {n_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T07:49:47.486340Z",
     "start_time": "2020-11-29T07:49:47.482346Z"
    }
   },
   "outputs": [],
   "source": [
    "alpha = 4.0\n",
    "factor = 0.5\n",
    "n_seeds = 5\n",
    "n_splits = 5\n",
    "patience = 30\n",
    "shuffle = True\n",
    "fit_params = {\"epochs\": 1_000, \"verbose\": 0}\n",
    "\n",
    "stacked_tabnet_params = dict(\n",
    "    epsilon=1e-05,\n",
    "    feature_columns=None,\n",
    "    virtual_batch_size=None,\n",
    "    num_layers=2,\n",
    "    num_decision_steps=1,\n",
    "    norm_type=\"batch\",\n",
    "    num_groups=-1,\n",
    "    batch_momentum=0.9,\n",
    "    relaxation_factor=1.2,\n",
    "    sparsity_coefficient=0.0001,\n",
    "    feature_dim=2560,\n",
    "    output_dim=128,\n",
    ")\n",
    "\n",
    "#DEBUG = True\n",
    "DEBUG = False\n",
    "if DEBUG:\n",
    "    n_seeds = 2\n",
    "    n_splits = 2\n",
    "    fit_params = {\"epochs\": 2, \"verbose\": 1}\n",
    "    print(\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T07:49:49.910171Z",
     "start_time": "2020-11-29T07:49:49.893215Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "start_predictors = [\n",
    "    \"g-0\",\n",
    "    \"g-7\",\n",
    "    \"g-8\",\n",
    "    \"g-10\",\n",
    "    \"g-13\",\n",
    "    \"g-17\",\n",
    "    \"g-20\",\n",
    "    \"g-22\",\n",
    "    \"g-24\",\n",
    "    \"g-26\",\n",
    "    \"g-28\",\n",
    "    \"g-29\",\n",
    "    \"g-30\",\n",
    "    \"g-31\",\n",
    "    \"g-32\",\n",
    "    \"g-34\",\n",
    "    \"g-35\",\n",
    "    \"g-36\",\n",
    "    \"g-37\",\n",
    "    \"g-38\",\n",
    "    \"g-39\",\n",
    "    \"g-41\",\n",
    "    \"g-46\",\n",
    "    \"g-48\",\n",
    "    \"g-50\",\n",
    "    \"g-51\",\n",
    "    \"g-52\",\n",
    "    \"g-55\",\n",
    "    \"g-58\",\n",
    "    \"g-59\",\n",
    "    \"g-61\",\n",
    "    \"g-62\",\n",
    "    \"g-63\",\n",
    "    \"g-65\",\n",
    "    \"g-66\",\n",
    "    \"g-67\",\n",
    "    \"g-68\",\n",
    "    \"g-70\",\n",
    "    \"g-72\",\n",
    "    \"g-74\",\n",
    "    \"g-75\",\n",
    "    \"g-79\",\n",
    "    \"g-83\",\n",
    "    \"g-84\",\n",
    "    \"g-85\",\n",
    "    \"g-86\",\n",
    "    \"g-90\",\n",
    "    \"g-91\",\n",
    "    \"g-94\",\n",
    "    \"g-95\",\n",
    "    \"g-96\",\n",
    "    \"g-97\",\n",
    "    \"g-98\",\n",
    "    \"g-100\",\n",
    "    \"g-102\",\n",
    "    \"g-105\",\n",
    "    \"g-106\",\n",
    "    \"g-112\",\n",
    "    \"g-113\",\n",
    "    \"g-114\",\n",
    "    \"g-116\",\n",
    "    \"g-121\",\n",
    "    \"g-123\",\n",
    "    \"g-126\",\n",
    "    \"g-128\",\n",
    "    \"g-131\",\n",
    "    \"g-132\",\n",
    "    \"g-134\",\n",
    "    \"g-135\",\n",
    "    \"g-138\",\n",
    "    \"g-139\",\n",
    "    \"g-140\",\n",
    "    \"g-142\",\n",
    "    \"g-144\",\n",
    "    \"g-145\",\n",
    "    \"g-146\",\n",
    "    \"g-147\",\n",
    "    \"g-148\",\n",
    "    \"g-152\",\n",
    "    \"g-155\",\n",
    "    \"g-157\",\n",
    "    \"g-158\",\n",
    "    \"g-160\",\n",
    "    \"g-163\",\n",
    "    \"g-164\",\n",
    "    \"g-165\",\n",
    "    \"g-170\",\n",
    "    \"g-173\",\n",
    "    \"g-174\",\n",
    "    \"g-175\",\n",
    "    \"g-177\",\n",
    "    \"g-178\",\n",
    "    \"g-181\",\n",
    "    \"g-183\",\n",
    "    \"g-185\",\n",
    "    \"g-186\",\n",
    "    \"g-189\",\n",
    "    \"g-192\",\n",
    "    \"g-194\",\n",
    "    \"g-195\",\n",
    "    \"g-196\",\n",
    "    \"g-197\",\n",
    "    \"g-199\",\n",
    "    \"g-201\",\n",
    "    \"g-202\",\n",
    "    \"g-206\",\n",
    "    \"g-208\",\n",
    "    \"g-210\",\n",
    "    \"g-213\",\n",
    "    \"g-214\",\n",
    "    \"g-215\",\n",
    "    \"g-220\",\n",
    "    \"g-226\",\n",
    "    \"g-228\",\n",
    "    \"g-229\",\n",
    "    \"g-235\",\n",
    "    \"g-238\",\n",
    "    \"g-241\",\n",
    "    \"g-242\",\n",
    "    \"g-243\",\n",
    "    \"g-244\",\n",
    "    \"g-245\",\n",
    "    \"g-248\",\n",
    "    \"g-250\",\n",
    "    \"g-251\",\n",
    "    \"g-254\",\n",
    "    \"g-257\",\n",
    "    \"g-259\",\n",
    "    \"g-261\",\n",
    "    \"g-266\",\n",
    "    \"g-270\",\n",
    "    \"g-271\",\n",
    "    \"g-272\",\n",
    "    \"g-275\",\n",
    "    \"g-278\",\n",
    "    \"g-282\",\n",
    "    \"g-287\",\n",
    "    \"g-288\",\n",
    "    \"g-289\",\n",
    "    \"g-291\",\n",
    "    \"g-293\",\n",
    "    \"g-294\",\n",
    "    \"g-297\",\n",
    "    \"g-298\",\n",
    "    \"g-301\",\n",
    "    \"g-303\",\n",
    "    \"g-304\",\n",
    "    \"g-306\",\n",
    "    \"g-308\",\n",
    "    \"g-309\",\n",
    "    \"g-310\",\n",
    "    \"g-311\",\n",
    "    \"g-314\",\n",
    "    \"g-315\",\n",
    "    \"g-316\",\n",
    "    \"g-317\",\n",
    "    \"g-320\",\n",
    "    \"g-321\",\n",
    "    \"g-322\",\n",
    "    \"g-327\",\n",
    "    \"g-328\",\n",
    "    \"g-329\",\n",
    "    \"g-332\",\n",
    "    \"g-334\",\n",
    "    \"g-335\",\n",
    "    \"g-336\",\n",
    "    \"g-337\",\n",
    "    \"g-339\",\n",
    "    \"g-342\",\n",
    "    \"g-344\",\n",
    "    \"g-349\",\n",
    "    \"g-350\",\n",
    "    \"g-351\",\n",
    "    \"g-353\",\n",
    "    \"g-354\",\n",
    "    \"g-355\",\n",
    "    \"g-357\",\n",
    "    \"g-359\",\n",
    "    \"g-360\",\n",
    "    \"g-364\",\n",
    "    \"g-365\",\n",
    "    \"g-366\",\n",
    "    \"g-367\",\n",
    "    \"g-368\",\n",
    "    \"g-369\",\n",
    "    \"g-374\",\n",
    "    \"g-375\",\n",
    "    \"g-377\",\n",
    "    \"g-379\",\n",
    "    \"g-385\",\n",
    "    \"g-386\",\n",
    "    \"g-390\",\n",
    "    \"g-392\",\n",
    "    \"g-393\",\n",
    "    \"g-400\",\n",
    "    \"g-402\",\n",
    "    \"g-406\",\n",
    "    \"g-407\",\n",
    "    \"g-409\",\n",
    "    \"g-410\",\n",
    "    \"g-411\",\n",
    "    \"g-414\",\n",
    "    \"g-417\",\n",
    "    \"g-418\",\n",
    "    \"g-421\",\n",
    "    \"g-423\",\n",
    "    \"g-424\",\n",
    "    \"g-427\",\n",
    "    \"g-429\",\n",
    "    \"g-431\",\n",
    "    \"g-432\",\n",
    "    \"g-433\",\n",
    "    \"g-434\",\n",
    "    \"g-437\",\n",
    "    \"g-439\",\n",
    "    \"g-440\",\n",
    "    \"g-443\",\n",
    "    \"g-449\",\n",
    "    \"g-458\",\n",
    "    \"g-459\",\n",
    "    \"g-460\",\n",
    "    \"g-461\",\n",
    "    \"g-464\",\n",
    "    \"g-467\",\n",
    "    \"g-468\",\n",
    "    \"g-470\",\n",
    "    \"g-473\",\n",
    "    \"g-477\",\n",
    "    \"g-478\",\n",
    "    \"g-479\",\n",
    "    \"g-484\",\n",
    "    \"g-485\",\n",
    "    \"g-486\",\n",
    "    \"g-488\",\n",
    "    \"g-489\",\n",
    "    \"g-491\",\n",
    "    \"g-494\",\n",
    "    \"g-496\",\n",
    "    \"g-498\",\n",
    "    \"g-500\",\n",
    "    \"g-503\",\n",
    "    \"g-504\",\n",
    "    \"g-506\",\n",
    "    \"g-508\",\n",
    "    \"g-509\",\n",
    "    \"g-512\",\n",
    "    \"g-522\",\n",
    "    \"g-529\",\n",
    "    \"g-531\",\n",
    "    \"g-534\",\n",
    "    \"g-539\",\n",
    "    \"g-541\",\n",
    "    \"g-546\",\n",
    "    \"g-551\",\n",
    "    \"g-553\",\n",
    "    \"g-554\",\n",
    "    \"g-559\",\n",
    "    \"g-561\",\n",
    "    \"g-562\",\n",
    "    \"g-565\",\n",
    "    \"g-568\",\n",
    "    \"g-569\",\n",
    "    \"g-574\",\n",
    "    \"g-577\",\n",
    "    \"g-578\",\n",
    "    \"g-586\",\n",
    "    \"g-588\",\n",
    "    \"g-590\",\n",
    "    \"g-594\",\n",
    "    \"g-595\",\n",
    "    \"g-596\",\n",
    "    \"g-597\",\n",
    "    \"g-599\",\n",
    "    \"g-600\",\n",
    "    \"g-603\",\n",
    "    \"g-607\",\n",
    "    \"g-615\",\n",
    "    \"g-618\",\n",
    "    \"g-619\",\n",
    "    \"g-620\",\n",
    "    \"g-625\",\n",
    "    \"g-628\",\n",
    "    \"g-629\",\n",
    "    \"g-632\",\n",
    "    \"g-634\",\n",
    "    \"g-635\",\n",
    "    \"g-636\",\n",
    "    \"g-638\",\n",
    "    \"g-639\",\n",
    "    \"g-641\",\n",
    "    \"g-643\",\n",
    "    \"g-644\",\n",
    "    \"g-645\",\n",
    "    \"g-646\",\n",
    "    \"g-647\",\n",
    "    \"g-648\",\n",
    "    \"g-663\",\n",
    "    \"g-664\",\n",
    "    \"g-665\",\n",
    "    \"g-668\",\n",
    "    \"g-669\",\n",
    "    \"g-670\",\n",
    "    \"g-671\",\n",
    "    \"g-672\",\n",
    "    \"g-673\",\n",
    "    \"g-674\",\n",
    "    \"g-677\",\n",
    "    \"g-678\",\n",
    "    \"g-680\",\n",
    "    \"g-683\",\n",
    "    \"g-689\",\n",
    "    \"g-691\",\n",
    "    \"g-693\",\n",
    "    \"g-695\",\n",
    "    \"g-701\",\n",
    "    \"g-702\",\n",
    "    \"g-703\",\n",
    "    \"g-704\",\n",
    "    \"g-705\",\n",
    "    \"g-706\",\n",
    "    \"g-708\",\n",
    "    \"g-711\",\n",
    "    \"g-712\",\n",
    "    \"g-720\",\n",
    "    \"g-721\",\n",
    "    \"g-723\",\n",
    "    \"g-724\",\n",
    "    \"g-726\",\n",
    "    \"g-728\",\n",
    "    \"g-731\",\n",
    "    \"g-733\",\n",
    "    \"g-738\",\n",
    "    \"g-739\",\n",
    "    \"g-742\",\n",
    "    \"g-743\",\n",
    "    \"g-744\",\n",
    "    \"g-745\",\n",
    "    \"g-749\",\n",
    "    \"g-750\",\n",
    "    \"g-752\",\n",
    "    \"g-760\",\n",
    "    \"g-761\",\n",
    "    \"g-764\",\n",
    "    \"g-766\",\n",
    "    \"g-768\",\n",
    "    \"g-770\",\n",
    "    \"g-771\",\n",
    "    \"c-0\",\n",
    "    \"c-1\",\n",
    "    \"c-2\",\n",
    "    \"c-3\",\n",
    "    \"c-4\",\n",
    "    \"c-5\",\n",
    "    \"c-6\",\n",
    "    \"c-7\",\n",
    "    \"c-8\",\n",
    "    \"c-9\",\n",
    "    \"c-10\",\n",
    "    \"c-11\",\n",
    "    \"c-12\",\n",
    "    \"c-13\",\n",
    "    \"c-14\",\n",
    "    \"c-15\",\n",
    "    \"c-16\",\n",
    "    \"c-17\",\n",
    "    \"c-18\",\n",
    "    \"c-19\",\n",
    "    \"c-20\",\n",
    "    \"c-21\",\n",
    "    \"c-22\",\n",
    "    \"c-23\",\n",
    "    \"c-24\",\n",
    "    \"c-25\",\n",
    "    \"c-26\",\n",
    "    \"c-27\",\n",
    "    \"c-28\",\n",
    "    \"c-29\",\n",
    "    \"c-30\",\n",
    "    \"c-31\",\n",
    "    \"c-32\",\n",
    "    \"c-33\",\n",
    "    \"c-34\",\n",
    "    \"c-35\",\n",
    "    \"c-36\",\n",
    "    \"c-37\",\n",
    "    \"c-38\",\n",
    "    \"c-39\",\n",
    "    \"c-40\",\n",
    "    \"c-41\",\n",
    "    \"c-42\",\n",
    "    \"c-43\",\n",
    "    \"c-44\",\n",
    "    \"c-45\",\n",
    "    \"c-46\",\n",
    "    \"c-47\",\n",
    "    \"c-48\",\n",
    "    \"c-49\",\n",
    "    \"c-50\",\n",
    "    \"c-51\",\n",
    "    \"c-52\",\n",
    "    \"c-53\",\n",
    "    \"c-54\",\n",
    "    \"c-55\",\n",
    "    \"c-56\",\n",
    "    \"c-57\",\n",
    "    \"c-58\",\n",
    "    \"c-59\",\n",
    "    \"c-60\",\n",
    "    \"c-61\",\n",
    "    \"c-62\",\n",
    "    \"c-63\",\n",
    "    \"c-64\",\n",
    "    \"c-65\",\n",
    "    \"c-66\",\n",
    "    \"c-67\",\n",
    "    \"c-68\",\n",
    "    \"c-69\",\n",
    "    \"c-70\",\n",
    "    \"c-71\",\n",
    "    \"c-72\",\n",
    "    \"c-73\",\n",
    "    \"c-74\",\n",
    "    \"c-75\",\n",
    "    \"c-76\",\n",
    "    \"c-77\",\n",
    "    \"c-78\",\n",
    "    \"c-79\",\n",
    "    \"c-80\",\n",
    "    \"c-81\",\n",
    "    \"c-82\",\n",
    "    \"c-83\",\n",
    "    \"c-84\",\n",
    "    \"c-85\",\n",
    "    \"c-86\",\n",
    "    \"c-87\",\n",
    "    \"c-88\",\n",
    "    \"c-89\",\n",
    "    \"c-90\",\n",
    "    \"c-91\",\n",
    "    \"c-92\",\n",
    "    \"c-93\",\n",
    "    \"c-94\",\n",
    "    \"c-95\",\n",
    "    \"c-96\",\n",
    "    \"c-97\",\n",
    "    \"c-98\",\n",
    "    \"c-99\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T07:50:58.045170Z",
     "start_time": "2020-11-29T07:50:58.033197Z"
    },
    "code_folding": [],
    "execution": {
     "iopub.execute_input": "2020-11-15T04:40:27.492218Z",
     "iopub.status.busy": "2020-11-15T04:40:27.491480Z",
     "iopub.status.idle": "2020-11-15T10:01:54.657676Z",
     "shell.execute_reply": "2020-11-15T10:01:54.658434Z"
    },
    "papermill": {
     "duration": 19287.2044,
     "end_time": "2020-11-15T10:01:54.658768",
     "exception": false,
     "start_time": "2020-11-15T04:40:27.454368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(batch_size, model_type=\"rs\", params={}):\n",
    "    print(f\"\\n{model_type}\")\n",
    "    counts = np.empty((n_seeds * n_splits, n_classes))\n",
    "\n",
    "    bias_initializer = -Y.mean(axis=0).apply(np.log).values\n",
    "    bias_initializer = tf.keras.initializers.Constant(bias_initializer)\n",
    "\n",
    "    Y_pred = np.zeros((train_size, n_classes))\n",
    "    Y_pred = pd.DataFrame(Y_pred, columns=Y.columns, index=Y.index)\n",
    "\n",
    "    for i in range(n_seeds):\n",
    "    #for i in [3,4]:  # gpu死んだので途中から再実行\n",
    "        print(f\"\\n---------- seed: {i} ----------\")\n",
    "        set_seed(seed=i)\n",
    "\n",
    "        cv = MultilabelStratifiedGroupKFold(\n",
    "            n_splits=n_splits, random_state=i, shuffle=shuffle\n",
    "        )\n",
    "\n",
    "        for j, (train, valid) in enumerate(cv.split(X, Y[columns], groups)):\n",
    "            print(f\"\\n================= fold: {j} =================\")\n",
    "            counts[i * n_splits + j] = Y.iloc[train].sum()\n",
    "\n",
    "            os.makedirs(model_type, exist_ok=True)\n",
    "\n",
    "            # model_nonscored_path = f\"model_nonscored_seed_{i}_fold_{j}.h5\"\n",
    "            model_path = f\"{model_type}/model_seed_{i}_fold_{j}.h5\"\n",
    "\n",
    "            K.clear_session()\n",
    "            if model_type == \"5l\":\n",
    "                model = create_model_5l(n_features, num_classes=n_classes, **params)\n",
    "            elif model_type == \"4l\":\n",
    "                model = create_model_4l(n_features, num_classes=n_classes, **params)\n",
    "            elif model_type == \"3l_v2\":\n",
    "                model = create_model_3l_v2(n_features, num_classes=n_classes, **params)\n",
    "            elif model_type == \"2l\":\n",
    "                model = create_model_2l(n_features, num_classes=n_classes, **params)\n",
    "            elif model_type == \"rs\":\n",
    "                model = create_model_rs(\n",
    "                    n_features, len(start_predictors), num_classes=n_classes, **params\n",
    "                )\n",
    "            elif model_type == \"StackedTabNet\":\n",
    "                model = create_model_stacked_tabnet(n_features, num_classes=n_classes)\n",
    "\n",
    "            if model_type == \"rs\":\n",
    "                # 入力2つのNN使うから工夫してる\n",
    "                X_ = X[start_predictors]\n",
    "\n",
    "                callbacks = build_callbacks(\n",
    "                    model_path, factor=factor, patience=patience\n",
    "                )\n",
    "#                history = model.fit(\n",
    "#                    [X.iloc[train], X_.iloc[train]],\n",
    "#                    Y.iloc[train],\n",
    "#                    batch_size=batch_size,\n",
    "#                    callbacks=callbacks,\n",
    "#                    validation_data=([X.iloc[valid], X_.iloc[valid]], Y.iloc[valid]),\n",
    "#                    **fit_params,\n",
    "#                )\n",
    "\n",
    "                model.load_weights(model_path)\n",
    "\n",
    "                Y_pred.iloc[valid] += (\n",
    "                    model.predict([X.iloc[valid], X_.iloc[valid]]) / n_seeds\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                generator = Cutmix(\n",
    "                    X.iloc[train], Y.iloc[train], alpha=alpha, batch_size=batch_size\n",
    "                )\n",
    "                callbacks = build_callbacks(\n",
    "                    model_path, factor=factor, patience=patience\n",
    "                )\n",
    "                history = model.fit(\n",
    "                    generator,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=(X.iloc[valid], Y.iloc[valid]),\n",
    "                    **fit_params,\n",
    "                )\n",
    "\n",
    "                model.load_weights(model_path)\n",
    "\n",
    "                Y_pred.iloc[valid] += model.predict(X.iloc[valid]) / n_seeds\n",
    "\n",
    "            del model\n",
    "            gc.collect()\n",
    "                \n",
    "    Y_pred[train_features[\"cp_type\"] == \"ctl_vehicle\"] = 0.0\n",
    "\n",
    "    with open(\"counts.pkl\", \"wb\") as f:\n",
    "        pickle.dump(counts, f)\n",
    "\n",
    "    with open(f\"Y_pred_{model_type}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(Y_pred[columns], f)\n",
    "\n",
    "    oof_score = score(Y[columns], Y_pred[columns])\n",
    "    print(f\"oof_score: {oof_score}\")\n",
    "\n",
    "    oof_auc_score = auc_score(Y[columns], Y_pred[columns])\n",
    "    print(f\"oof_auc_score: {oof_auc_score}\")\n",
    "\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    return oof_score, Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-28T05:41:23.321Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass shuffle=True, random_state=1 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass shuffle=True, random_state=2 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass shuffle=True, random_state=3 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if __name__ == \"__main__\":\n",
    "    # MLPs\n",
    "    #oof_score, Y_pred = train_and_evaluate(model_type=\"5l\", batch_size=32)\n",
    "    #oof_score, Y_pred = train_and_evaluate(model_type=\"4l\", batch_size=32)\n",
    "    #oof_score, Y_pred = train_and_evaluate(model_type=\"3l_v2\", batch_size=32)\n",
    "    #oof_score, Y_pred = train_and_evaluate(model_type=\"2l\", batch_size=32)\n",
    "    oof_score, Y_pred = train_and_evaluate(model_type=\"rs\", batch_size=32)\n",
    "\n",
    "    # Stacked TabNet\n",
    "    #oof_score, Y_pred = train_and_evaluate(model_type=\"StackedTabNet\", batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T07:13:47.131537Z",
     "start_time": "2020-11-29T03:46:28.407582Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "rs\n",
      "\n",
      "---------- seed: 3 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass shuffle=True, random_state=3 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= fold: 0 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "================= fold: 1 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "================= fold: 2 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "================= fold: 3 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "================= fold: 4 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "---------- seed: 4 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass shuffle=True, random_state=4 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= fold: 0 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "================= fold: 1 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "================= fold: 2 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "================= fold: 3 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "================= fold: 4 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "oof_score: 0.017103339890134007\n",
      "oof_auc_score: 0.6783249890248004\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Wall time: 3h 27min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# gpu死んだので途中から再実行\n",
    "if __name__ == \"__main__\":\n",
    "    oof_score, Y_pred = train_and_evaluate(model_type=\"rs\", batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T07:52:01.263292Z",
     "start_time": "2020-11-29T07:51:03.093954Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "rs\n",
      "\n",
      "---------- seed: 0 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= fold: 0 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "================= fold: 1 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "================= fold: 2 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "================= fold: 3 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "================= fold: 4 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "---------- seed: 1 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass shuffle=True, random_state=1 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= fold: 0 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "================= fold: 1 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "================= fold: 2 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "================= fold: 3 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "================= fold: 4 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "---------- seed: 2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass shuffle=True, random_state=2 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= fold: 0 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "================= fold: 1 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "================= fold: 2 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "================= fold: 3 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "================= fold: 4 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "---------- seed: 3 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass shuffle=True, random_state=3 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= fold: 0 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "================= fold: 1 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "================= fold: 2 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "================= fold: 3 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "================= fold: 4 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "---------- seed: 4 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass shuffle=True, random_state=4 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= fold: 0 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "================= fold: 1 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "================= fold: 2 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "================= fold: 3 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "\n",
      "================= fold: 4 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "oof_score: 0.01588816124967386\n",
      "oof_auc_score: 0.6780838531221702\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Wall time: 58.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 学習処理コメントアウトしてoof確認\n",
    "if __name__ == \"__main__\":\n",
    "    oof_score, Y_pred = train_and_evaluate(model_type=\"rs\", batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.6.0"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "papermill": {
   "duration": 19308.710664,
   "end_time": "2020-11-15T10:01:56.008945",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-15T04:40:07.298281",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
