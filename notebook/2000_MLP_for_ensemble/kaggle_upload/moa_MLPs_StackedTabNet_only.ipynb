{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考: https://www.kaggle.com/yxohrxn/mlpclassifier-fit?scriptVersionId=46905918"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T05:57:42.473754Z",
     "start_time": "2020-11-29T05:57:42.470761Z"
    },
    "execution": {
     "iopub.execute_input": "2020-11-15T04:40:11.867203Z",
     "iopub.status.busy": "2020-11-15T04:40:11.866191Z",
     "iopub.status.idle": "2020-11-15T04:40:11.869103Z",
     "shell.execute_reply": "2020-11-15T04:40:11.868399Z"
    },
    "papermill": {
     "duration": 0.029988,
     "end_time": "2020-11-15T04:40:11.869223",
     "exception": false,
     "start_time": "2020-11-15T04:40:11.839235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# sys.path.append(\"../input/adabeliefoptimizer/pypi_packages/adabelief_tf0.1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T05:57:43.336605Z",
     "start_time": "2020-11-29T05:57:43.333613Z"
    },
    "execution": {
     "iopub.execute_input": "2020-11-15T04:40:11.916751Z",
     "iopub.status.busy": "2020-11-15T04:40:11.915736Z",
     "iopub.status.idle": "2020-11-15T04:40:11.919098Z",
     "shell.execute_reply": "2020-11-15T04:40:11.918390Z"
    },
    "papermill": {
     "duration": 0.028727,
     "end_time": "2020-11-15T04:40:11.919213",
     "exception": false,
     "start_time": "2020-11-15T04:40:11.890486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# sys.path.append(\"../input/iterative-stratification/iterative-stratification-master\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T05:57:44.165611Z",
     "start_time": "2020-11-29T05:57:44.162619Z"
    }
   },
   "outputs": [],
   "source": [
    "# GPUメモリ必要な分だけ確保\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T05:57:44.925220Z",
     "start_time": "2020-11-29T05:57:44.922212Z"
    }
   },
   "outputs": [],
   "source": [
    "# # GPU使わない\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T08:21:29.909515Z",
     "start_time": "2020-11-29T08:21:26.490821Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-11-15T04:40:11.967087Z",
     "iopub.status.busy": "2020-11-15T04:40:11.966155Z",
     "iopub.status.idle": "2020-11-15T04:40:17.400927Z",
     "shell.execute_reply": "2020-11-15T04:40:17.400141Z"
    },
    "papermill": {
     "duration": 5.46079,
     "end_time": "2020-11-15T04:40:17.401049",
     "exception": false,
     "start_time": "2020-11-15T04:40:11.940259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T05:57:47.409341Z",
     "start_time": "2020-11-29T05:57:47.404044Z"
    },
    "code_folding": [],
    "execution": {
     "iopub.execute_input": "2020-11-15T04:40:17.455544Z",
     "iopub.status.busy": "2020-11-15T04:40:17.454466Z",
     "iopub.status.idle": "2020-11-15T04:40:17.457712Z",
     "shell.execute_reply": "2020-11-15T04:40:17.457108Z"
    },
    "papermill": {
     "duration": 0.034006,
     "end_time": "2020-11-15T04:40:17.457833",
     "exception": false,
     "start_time": "2020-11-15T04:40:17.423827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def build_callbacks(\n",
    "    model_path, factor=0.1, mode=\"auto\", monitor=\"val_loss\", patience=0, verbose=0\n",
    "):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        mode=mode, monitor=monitor, patience=patience, verbose=verbose\n",
    "    )\n",
    "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        model_path, mode=mode, monitor=monitor, save_best_only=True, verbose=verbose\n",
    "    )\n",
    "    reduce_lr_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        factor=factor, monitor=monitor, mode=mode, verbose=verbose\n",
    "    )\n",
    "\n",
    "    return [early_stopping, model_checkpoint, reduce_lr_on_plateau]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T05:57:47.479948Z",
     "start_time": "2020-11-29T05:57:47.410323Z"
    },
    "code_folding": [
     13,
     96,
     163,
     219,
     265
    ]
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from adabelief_tf import AdaBeliefOptimizer\n",
    "\n",
    "\n",
    "def create_model_rs(\n",
    "    shape1,\n",
    "    shape2,\n",
    "    num_classes=206,\n",
    "    lr=0.03,\n",
    "    activations=[\"selu\", \"selu\", \"selu\", \"selu\", \"selu\", \"selu\", \"relu\", \"selu\"],\n",
    "    denses=[369, 939, 696, 970, 887, 991],\n",
    "    drop_rates=[0.777298030070047, 0.6033124455518569],\n",
    "):\n",
    "    \"\"\"入力2つのNN.resnetみたくskip connection入れてる\"\"\"\n",
    "    input_1 = tf.keras.layers.Input(shape=(shape1))\n",
    "    input_2 = tf.keras.layers.Input(shape=(shape2))\n",
    "\n",
    "    head_1 = tf.keras.layers.BatchNormalization()(input_1)\n",
    "    head_1 = tf.keras.layers.Dropout(drop_rates[0])(head_1)\n",
    "    head_1 = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[0], activation=activations[0])\n",
    "    )(head_1)\n",
    "    head_1 = tf.keras.layers.BatchNormalization()(head_1)\n",
    "    input_3 = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[1], activation=activations[1])\n",
    "    )(head_1)\n",
    "\n",
    "    input_3_concat = tf.keras.layers.Concatenate()(\n",
    "        [input_2, input_3]\n",
    "    )  # node連結。node数が2つのnodeの足し算になる\n",
    "\n",
    "    head_2 = tf.keras.layers.BatchNormalization()(input_3_concat)\n",
    "    head_2 = tf.keras.layers.Dropout(drop_rates[1])(head_2)\n",
    "    head_2 = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[2], activations[2])\n",
    "    )(head_2)\n",
    "    head_2 = tf.keras.layers.BatchNormalization()(head_2)\n",
    "    head_2 = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[3], activations[3])\n",
    "    )(head_2)\n",
    "    head_2 = tf.keras.layers.BatchNormalization()(head_2)\n",
    "    head_2 = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[4], activations[4])\n",
    "    )(head_2)\n",
    "    head_2 = tf.keras.layers.BatchNormalization()(head_2)\n",
    "    input_4 = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[1], activations[5])\n",
    "    )(\n",
    "        head_2\n",
    "    )  # input_3 と同じnode数でないとだめ. tf.keras.layers.Average するから\n",
    "\n",
    "    input_4_avg = tf.keras.layers.Average()([input_3, input_4])  # 入力のリストを要素ごとに平均化\n",
    "\n",
    "    head_3 = tf.keras.layers.BatchNormalization()(input_4_avg)\n",
    "    head_3 = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(\n",
    "            denses[5], kernel_initializer=\"lecun_normal\", activation=activations[6]\n",
    "        )\n",
    "    )(head_3)\n",
    "    head_3 = tf.keras.layers.BatchNormalization()(head_3)\n",
    "    head_3 = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(\n",
    "            num_classes, kernel_initializer=\"lecun_normal\", activation=activations[7]\n",
    "        )\n",
    "    )(head_3)\n",
    "    head_3 = tf.keras.layers.BatchNormalization()(head_3)\n",
    "    output = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(num_classes, activation=\"sigmoid\")\n",
    "    )(head_3)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[input_1, input_2], outputs=output)\n",
    "    opt = AdaBeliefOptimizer(learning_rate=lr)\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.0015),  # ラベルスムージング\n",
    "        metrics=tf.keras.metrics.BinaryCrossentropy(),\n",
    "    )\n",
    "    # model.save(\"model/rs_shape.h5\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_5l(\n",
    "    shape,\n",
    "    num_classes=206,\n",
    "    lr=0.03,\n",
    "    activation=\"relu\",\n",
    "    denses=[512, 512, 512, 512, 512],\n",
    "    drop_rates=[\n",
    "        0.28593716228565935,\n",
    "        0.3271533962453585,\n",
    "        0.5271338848921763,\n",
    "        0.2676199676497088,\n",
    "        0.5363898352447071,\n",
    "        0.4021564764052554,\n",
    "    ],\n",
    "):\n",
    "    \"\"\"入力1つの5層NN。Stochastic Weight Averaging使う\"\"\"\n",
    "    inp = tf.keras.layers.Input(shape=(shape))\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(inp)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[0])(x)\n",
    "    x = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[0], activation=activation)\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[1])(x)\n",
    "    x = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[1], activation=activation)\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[2])(x)\n",
    "    x = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[2], activation=activation)\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[3])(x)\n",
    "    x = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[3], activation=activation)\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[4])(x)\n",
    "    x = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[4], activation=activation)\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[5])(x)\n",
    "    out = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(num_classes, activation=\"sigmoid\")\n",
    "    )(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=out)\n",
    "    opt = tf.optimizers.Adam(learning_rate=lr)\n",
    "    opt = tfa.optimizers.SWA(\n",
    "        opt\n",
    "    )  # Stochastic Weight Averaging.モデルの重みを、これまで＋今回の平均を取って更新していくことでうまくいくみたい https://twitter.com/icoxfog417/status/989762534163992577\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.0020),  # ラベルスムージング\n",
    "        metrics=tf.keras.metrics.BinaryCrossentropy(),\n",
    "    )\n",
    "    # model.save(\"model/5l_shape.h5\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_4l(\n",
    "    shape,\n",
    "    num_classes=206,\n",
    "    lr=0.03,\n",
    "    activation=\"relu\",\n",
    "    denses=[512, 448, 384, 320],\n",
    "    drop_rates=[\n",
    "        0.20043759292966096,\n",
    "        0.25143314190089106,\n",
    "        0.4898856244488173,\n",
    "        0.25296015385762904,\n",
    "        0.338214056622176,\n",
    "    ],\n",
    "):\n",
    "    \"\"\"入力1つの4層NN。シンプル\"\"\"\n",
    "    inp = tf.keras.layers.Input(shape=(shape))\n",
    "    x = tf.keras.layers.BatchNormalization()(inp)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[0])(x)\n",
    "    x = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[0], activation=activation)\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[1])(x)\n",
    "    x = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[1], activation=activation)\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[2])(x)\n",
    "    x = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[2], activation=activation)\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[3])(x)\n",
    "    x = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[3], activation=activation)\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[4])(x)\n",
    "    out = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(num_classes, activation=\"sigmoid\")\n",
    "    )(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=out)\n",
    "    opt = tf.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.0020),  # ラベルスムージング\n",
    "        metrics=tf.keras.metrics.BinaryCrossentropy(),\n",
    "    )\n",
    "    # model.save(\"model/4l_shape.h5\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_3l_v2(\n",
    "    shape,\n",
    "    num_classes=206,\n",
    "    lr=0.03,\n",
    "    activation=\"selu\",\n",
    "    denses=[1024, 1024, 1024],\n",
    "    drop_rates=[\n",
    "        0.12742511520132901,\n",
    "        0.5367078212868925,\n",
    "        0.3914765737655165,\n",
    "        0.20302740274780187,\n",
    "    ],\n",
    "):\n",
    "    \"\"\"入力1つの3層NN。adabelief_tf 使う\"\"\"\n",
    "    inp = tf.keras.layers.Input(shape=(shape))\n",
    "    x = tf.keras.layers.BatchNormalization()(inp)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[0])(x)\n",
    "    x = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[0], activation=activation)\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[1])(x)\n",
    "    x = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[1], activation=activation)\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[2])(x)\n",
    "    x = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[2], activation=activation)\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[3])(x)\n",
    "    out = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(num_classes, activation=\"sigmoid\")\n",
    "    )(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=out)\n",
    "    opt = AdaBeliefOptimizer(learning_rate=lr)\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.0015),\n",
    "        metrics=tf.keras.metrics.BinaryCrossentropy(),\n",
    "    )\n",
    "    # model.save(\"model/3l_shape.h5\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_2l(\n",
    "    shape,\n",
    "    num_classes=206,\n",
    "    lr=0.03,\n",
    "    activation=\"relu\",\n",
    "    denses=[1150, 1371],\n",
    "    drop_rates=[0.19506281094918945, 0.22193390554785974, 0.7161410458761702],\n",
    "    sync_period=38,\n",
    "):\n",
    "    \"\"\"入力1つの2層NN。Lookahead, WeightNormalization使う\"\"\"\n",
    "    inp = tf.keras.layers.Input(shape=(shape))\n",
    "    x = tf.keras.layers.BatchNormalization()(inp)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[0])(x)\n",
    "    x = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[0], activation=activation)\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[1])(x)\n",
    "    x = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(denses[1], activation=activation)\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(drop_rates[2])(x)\n",
    "    out = tfa.layers.WeightNormalization(\n",
    "        tf.keras.layers.Dense(num_classes, activation=\"sigmoid\")\n",
    "    )(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=out)\n",
    "    opt = tf.optimizers.Adam(learning_rate=lr)\n",
    "    opt = tfa.optimizers.Lookahead(opt, sync_period=sync_period)\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.0015),\n",
    "        metrics=tf.keras.metrics.BinaryCrossentropy(),\n",
    "    )\n",
    "    # model.save(\"model/2l_shape.h5\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T05:57:48.188016Z",
     "start_time": "2020-11-29T05:57:48.178015Z"
    },
    "code_folding": [
     11,
     29
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.3.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(r\"C:\\Users\\81908\\jupyter_notebook\\poetry_work\\tfgpu\\01_MoA_compe\\code\")\n",
    "from tabnet_tf import *\n",
    "\n",
    "# from tabnet import StackedTabNet\n",
    "\n",
    "import tensorflow as tf\n",
    "from adabelief_tf import AdaBeliefOptimizer\n",
    "\n",
    "\n",
    "class StackedTabNetClassifier(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        batch_momentum=0.98,\n",
    "        epsilon=1e-05,\n",
    "        feature_columns=None,\n",
    "        feature_dim=64,\n",
    "        norm_type=\"group\",\n",
    "        num_decision_steps=5,\n",
    "        num_features=None,\n",
    "        num_groups=2,\n",
    "        num_layers=1,\n",
    "        output_dim=64,\n",
    "        relaxation_factor=1.5,\n",
    "        sparsity_coefficient=1e-05,\n",
    "        virtual_batch_size=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.stacked_tabnet = StackedTabNet(\n",
    "            feature_columns,\n",
    "            batch_momentum=batch_momentum,\n",
    "            epsilon=epsilon,\n",
    "            feature_dim=feature_dim,\n",
    "            norm_type=norm_type,\n",
    "            num_decision_steps=num_decision_steps,\n",
    "            num_features=num_features,\n",
    "            num_groups=num_groups,\n",
    "            num_layers=num_layers,\n",
    "            output_dim=output_dim,\n",
    "            relaxation_factor=relaxation_factor,\n",
    "            sparsity_coefficient=sparsity_coefficient,\n",
    "            virtual_batch_size=virtual_batch_size,\n",
    "        )\n",
    "\n",
    "        self.classifier = tf.keras.layers.Dense(\n",
    "            num_classes, activation=\"sigmoid\", use_bias=False\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.stacked_tabnet(inputs, training=training)\n",
    "\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "def create_model_stacked_tabnet(\n",
    "    n_features, num_classes=206, lr=0.001,\n",
    "):\n",
    "    model = StackedTabNetClassifier(\n",
    "        num_classes=num_classes, num_features=n_features, **stacked_tabnet_params,\n",
    "    )\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=1e-03)\n",
    "    optimizer = AdaBeliefOptimizer(learning_rate=lr)\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T05:57:49.320757Z",
     "start_time": "2020-11-29T05:57:49.067448Z"
    },
    "code_folding": [
     3
    ],
    "execution": {
     "iopub.execute_input": "2020-11-15T04:40:18.021251Z",
     "iopub.status.busy": "2020-11-15T04:40:18.020509Z",
     "iopub.status.idle": "2020-11-15T04:40:18.794904Z",
     "shell.execute_reply": "2020-11-15T04:40:18.794072Z"
    },
    "papermill": {
     "duration": 0.805422,
     "end_time": "2020-11-15T04:40:18.795031",
     "exception": false,
     "start_time": "2020-11-15T04:40:17.989609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "def score(Y, Y_pred):\n",
    "    _, n_classes = Y.shape\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for j in range(n_classes):\n",
    "        loss = log_loss(Y.iloc[:, j], Y_pred.iloc[:, j], labels=[0, 1])\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T05:57:49.926187Z",
     "start_time": "2020-11-29T05:57:49.922198Z"
    },
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def auc_score(Y, Y_pred):\n",
    "    _, n_classes = Y.shape\n",
    "\n",
    "    aucs = []\n",
    "\n",
    "    for j in range(n_classes):\n",
    "        auc = roc_auc_score(Y.iloc[:, j], Y_pred.iloc[:, j])\n",
    "\n",
    "        aucs.append(auc)\n",
    "\n",
    "    return np.mean(aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T05:57:50.796650Z",
     "start_time": "2020-11-29T05:57:50.791663Z"
    },
    "code_folding": [
     7
    ],
    "execution": {
     "iopub.execute_input": "2020-11-15T04:40:18.852597Z",
     "iopub.status.busy": "2020-11-15T04:40:18.851571Z",
     "iopub.status.idle": "2020-11-15T04:40:18.855082Z",
     "shell.execute_reply": "2020-11-15T04:40:18.854311Z"
    },
    "papermill": {
     "duration": 0.036662,
     "end_time": "2020-11-15T04:40:18.855208",
     "exception": false,
     "start_time": "2020-11-15T04:40:18.818546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random as rn\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "    rn.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    graph = tf.compat.v1.get_default_graph()\n",
    "    session_conf = tf.compat.v1.ConfigProto(\n",
    "        inter_op_parallelism_threads=1, intra_op_parallelism_threads=1\n",
    "    )\n",
    "    sess = tf.compat.v1.Session(graph=graph, config=session_conf)\n",
    "\n",
    "    tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T05:57:51.626282Z",
     "start_time": "2020-11-29T05:57:51.622267Z"
    },
    "code_folding": [
     4
    ],
    "execution": {
     "iopub.execute_input": "2020-11-15T04:40:18.913724Z",
     "iopub.status.busy": "2020-11-15T04:40:18.912896Z",
     "iopub.status.idle": "2020-11-15T04:40:18.916149Z",
     "shell.execute_reply": "2020-11-15T04:40:18.915459Z"
    },
    "papermill": {
     "duration": 0.037276,
     "end_time": "2020-11-15T04:40:18.916269",
     "exception": false,
     "start_time": "2020-11-15T04:40:18.878993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "\n",
    "class ClippedFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, copy=True, high=0.99, low=0.01):\n",
    "        self.copy = copy\n",
    "        self.high = high\n",
    "        self.low = low\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.data_max_ = X.quantile(q=self.high)\n",
    "        self.data_min_ = X.quantile(q=self.low)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.copy:\n",
    "            X = X.copy()\n",
    "\n",
    "        X.clip(self.data_min_, self.data_max_, axis=1, inplace=True)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T05:57:52.632415Z",
     "start_time": "2020-11-29T05:57:52.626413Z"
    },
    "incorrectly_encoded_metadata": "code_folding=[6] code_folding=[6] execution={\"iopub.execute_input\": \"2020-11-15T04:40:18.982852Z\", \"iopub.status.busy\": \"2020-11-15T04:40:18.981785Z\", \"iopub.status.idle\": \"2020-11-15T04:40:18.985199Z\", \"shell.execute_reply\": \"2020-11-15T04:40:18.984617Z\"} _kg_hide-input=false",
    "papermill": {
     "duration": 0.045366,
     "end_time": "2020-11-15T04:40:18.985320",
     "exception": false,
     "start_time": "2020-11-15T04:40:18.939954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://arxiv.org/abs/1905.04899\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class Cutmix(tf.keras.utils.Sequence):\n",
    "    def __init__(self, X, y=None, batch_size=32, alpha=1.0):\n",
    "        self.X = np.asarray(X)\n",
    "\n",
    "        if y is None:\n",
    "            self.y = y\n",
    "        else:\n",
    "            self.y = np.asarray(y)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        X_batch = self.X[i * self.batch_size : (i + 1) * self.batch_size]\n",
    "\n",
    "        n_samples, n_features = self.X.shape\n",
    "        batch_size = X_batch.shape[0]\n",
    "        shuffle = np.random.choice(n_samples, batch_size)\n",
    "\n",
    "        l = np.random.beta(self.alpha, self.alpha)\n",
    "        mask = np.random.choice([0.0, 1.0], size=n_features, p=[1.0 - l, l])\n",
    "        X_shuffle = self.X[shuffle]\n",
    "        X_batch = mask * X_batch + (1.0 - mask) * X_shuffle\n",
    "\n",
    "        if self.y is None:\n",
    "            return X_batch, None\n",
    "\n",
    "        y_batch = self.y[i * self.batch_size : (i + 1) * self.batch_size]\n",
    "        y_shuffle = self.y[shuffle]\n",
    "        y_batch = l * y_batch + (1.0 - l) * y_shuffle\n",
    "\n",
    "        return X_batch, y_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        n_samples = self.X.shape[0]\n",
    "\n",
    "        return int(np.ceil(n_samples / self.batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T05:57:54.810267Z",
     "start_time": "2020-11-29T05:57:54.797276Z"
    },
    "code_folding": [
     5
    ],
    "execution": {
     "iopub.execute_input": "2020-11-15T04:40:19.223235Z",
     "iopub.status.busy": "2020-11-15T04:40:19.222417Z",
     "iopub.status.idle": "2020-11-15T04:40:19.248983Z",
     "shell.execute_reply": "2020-11-15T04:40:19.248301Z"
    },
    "papermill": {
     "duration": 0.069383,
     "end_time": "2020-11-15T04:40:19.249102",
     "exception": false,
     "start_time": "2020-11-15T04:40:19.179719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.model_selection._split import _BaseKFold\n",
    "\n",
    "\n",
    "class MultilabelStratifiedGroupKFold(_BaseKFold):\n",
    "    def __init__(self, n_splits=5, random_state=None, shuffle=False):\n",
    "        super().__init__(n_splits=n_splits, random_state=random_state, shuffle=shuffle)\n",
    "\n",
    "    def _iter_test_indices(self, X=None, Y=None, groups=None):\n",
    "        cv = MultilabelStratifiedKFold(\n",
    "            n_splits=self.n_splits,\n",
    "            random_state=self.random_state,\n",
    "            shuffle=self.shuffle,\n",
    "        )\n",
    "\n",
    "        value_counts = groups.value_counts()\n",
    "        regluar_indices = value_counts.loc[\n",
    "            (value_counts == 6) | (value_counts == 12) | (value_counts == 18)\n",
    "        ].index.sort_values()\n",
    "        irregluar_indices = value_counts.loc[\n",
    "            (value_counts != 6) & (value_counts != 12) & (value_counts != 18)\n",
    "        ].index.sort_values()\n",
    "\n",
    "        group_to_fold = {}\n",
    "        tmp = Y.groupby(groups).mean().loc[regluar_indices]\n",
    "\n",
    "        for fold, (_, test) in enumerate(cv.split(tmp, tmp)):\n",
    "            group_to_fold.update({group: fold for group in tmp.index[test]})\n",
    "\n",
    "        sample_to_fold = {}\n",
    "        tmp = Y.loc[groups.isin(irregluar_indices)]\n",
    "\n",
    "        for fold, (_, test) in enumerate(cv.split(tmp, tmp)):\n",
    "            sample_to_fold.update({sample: fold for sample in tmp.index[test]})\n",
    "\n",
    "        folds = groups.map(group_to_fold)\n",
    "        is_na = folds.isna()\n",
    "        folds[is_na] = folds[is_na].index.map(sample_to_fold).values\n",
    "\n",
    "        for i in range(self.n_splits):\n",
    "            yield np.where(folds == i)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T05:58:00.062588Z",
     "start_time": "2020-11-29T05:57:57.191993Z"
    }
   },
   "outputs": [],
   "source": [
    "dtype = {\"cp_type\": \"category\", \"cp_dose\": \"category\"}\n",
    "index_col = \"sig_id\"\n",
    "\n",
    "sys.path.append(r\"C:\\Users\\81908\\jupyter_notebook\\poetry_work\\tfgpu\\01_MoA_compe\\code\")\n",
    "import datasets\n",
    "\n",
    "DATADIR = datasets.DATADIR\n",
    "\n",
    "groups = pd.read_csv(\n",
    "    f\"{DATADIR}/train_drug.csv\", dtype=dtype, index_col=index_col, squeeze=True\n",
    ")\n",
    "train_features = pd.read_csv(\n",
    "    f\"{DATADIR}/train_features.csv\", dtype=dtype, index_col=index_col\n",
    ")\n",
    "# X_test = pd.read_csv(f\"{DATADIR}/test_features.csv\", dtype=dtype, index_col=index_col)\n",
    "X = train_features.select_dtypes(\"number\")\n",
    "Y_nonscored = pd.read_csv(f\"{DATADIR}/train_targets_nonscored.csv\", index_col=index_col)\n",
    "Y = pd.read_csv(f\"{DATADIR}/train_targets_scored.csv\", index_col=index_col)\n",
    "\n",
    "columns = Y.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T05:58:01.772849Z",
     "start_time": "2020-11-29T05:58:00.063585Z"
    },
    "execution": {
     "iopub.execute_input": "2020-11-15T04:40:25.034407Z",
     "iopub.status.busy": "2020-11-15T04:40:25.033563Z",
     "iopub.status.idle": "2020-11-15T04:40:27.251916Z",
     "shell.execute_reply": "2020-11-15T04:40:27.251250Z"
    },
    "papermill": {
     "duration": 2.253691,
     "end_time": "2020-11-15T04:40:27.252051",
     "exception": false,
     "start_time": "2020-11-15T04:40:24.998360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clipped_features = ClippedFeatures()\n",
    "X = clipped_features.fit_transform(X)\n",
    "\n",
    "with open(\"clipped_features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(clipped_features, f)\n",
    "\n",
    "# c_prefix = \"c-\"\n",
    "# g_prefix = \"g-\"\n",
    "# c_columns = X.columns.str.startswith(c_prefix)\n",
    "# g_columns = X.columns.str.startswith(g_prefix)\n",
    "# X_c = compute_row_statistics(X.loc[:, c_columns], prefix=c_prefix)\n",
    "# X_g = compute_row_statistics(X.loc[:, g_columns], prefix=g_prefix)\n",
    "# X = pd.concat([X, X_c, X_g], axis=1)\n",
    "\n",
    "# Y_nonscored = Y_nonscored.loc[:, Y_nonscored.sum(axis=0) > 0]\n",
    "# Y = pd.concat([Y, Y_nonscored], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T05:58:01.776813Z",
     "start_time": "2020-11-29T05:58:01.773821Z"
    },
    "execution": {
     "iopub.execute_input": "2020-11-15T04:40:27.307722Z",
     "iopub.status.busy": "2020-11-15T04:40:27.306775Z",
     "iopub.status.idle": "2020-11-15T04:40:27.309988Z",
     "shell.execute_reply": "2020-11-15T04:40:27.309231Z"
    },
    "papermill": {
     "duration": 0.033192,
     "end_time": "2020-11-15T04:40:27.310108",
     "exception": false,
     "start_time": "2020-11-15T04:40:27.276916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_classes: 206\n"
     ]
    }
   ],
   "source": [
    "train_size, n_features = X.shape\n",
    "_, n_classes_nonscored = Y_nonscored.shape\n",
    "_, n_classes = Y.shape\n",
    "print(f\"n_classes: {n_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T08:21:34.276681Z",
     "start_time": "2020-11-29T08:21:34.267681Z"
    }
   },
   "outputs": [],
   "source": [
    "alpha = 4.0\n",
    "factor = 0.5\n",
    "n_seeds = 5\n",
    "n_splits = 5\n",
    "patience = 30\n",
    "shuffle = True\n",
    "fit_params = {\"epochs\": 1_000, \"verbose\": 0}\n",
    "\n",
    "stacked_tabnet_params = dict(\n",
    "    epsilon=1e-05,\n",
    "    feature_columns=None,\n",
    "    virtual_batch_size=None,\n",
    "    num_layers=2,\n",
    "    num_decision_steps=1,\n",
    "    norm_type=\"batch\",\n",
    "    num_groups=-1,\n",
    "    batch_momentum=0.9,\n",
    "    relaxation_factor=1.2,\n",
    "    sparsity_coefficient=0.0001,\n",
    "    feature_dim=2560,\n",
    "    output_dim=128,\n",
    ")\n",
    "with open(f\"stacked_tabnet_params.pkl\", \"wb\") as f:\n",
    "    pickle.dump(stacked_tabnet_params, f)\n",
    "\n",
    "#DEBUG = True\n",
    "DEBUG = False\n",
    "if DEBUG:\n",
    "    n_seeds = 2\n",
    "    n_splits = 2\n",
    "    fit_params = {\"epochs\": 2, \"verbose\": 1}\n",
    "    print(\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T05:58:02.711269Z",
     "start_time": "2020-11-29T05:58:02.697308Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "start_predictors = [\n",
    "    \"g-0\",\n",
    "    \"g-7\",\n",
    "    \"g-8\",\n",
    "    \"g-10\",\n",
    "    \"g-13\",\n",
    "    \"g-17\",\n",
    "    \"g-20\",\n",
    "    \"g-22\",\n",
    "    \"g-24\",\n",
    "    \"g-26\",\n",
    "    \"g-28\",\n",
    "    \"g-29\",\n",
    "    \"g-30\",\n",
    "    \"g-31\",\n",
    "    \"g-32\",\n",
    "    \"g-34\",\n",
    "    \"g-35\",\n",
    "    \"g-36\",\n",
    "    \"g-37\",\n",
    "    \"g-38\",\n",
    "    \"g-39\",\n",
    "    \"g-41\",\n",
    "    \"g-46\",\n",
    "    \"g-48\",\n",
    "    \"g-50\",\n",
    "    \"g-51\",\n",
    "    \"g-52\",\n",
    "    \"g-55\",\n",
    "    \"g-58\",\n",
    "    \"g-59\",\n",
    "    \"g-61\",\n",
    "    \"g-62\",\n",
    "    \"g-63\",\n",
    "    \"g-65\",\n",
    "    \"g-66\",\n",
    "    \"g-67\",\n",
    "    \"g-68\",\n",
    "    \"g-70\",\n",
    "    \"g-72\",\n",
    "    \"g-74\",\n",
    "    \"g-75\",\n",
    "    \"g-79\",\n",
    "    \"g-83\",\n",
    "    \"g-84\",\n",
    "    \"g-85\",\n",
    "    \"g-86\",\n",
    "    \"g-90\",\n",
    "    \"g-91\",\n",
    "    \"g-94\",\n",
    "    \"g-95\",\n",
    "    \"g-96\",\n",
    "    \"g-97\",\n",
    "    \"g-98\",\n",
    "    \"g-100\",\n",
    "    \"g-102\",\n",
    "    \"g-105\",\n",
    "    \"g-106\",\n",
    "    \"g-112\",\n",
    "    \"g-113\",\n",
    "    \"g-114\",\n",
    "    \"g-116\",\n",
    "    \"g-121\",\n",
    "    \"g-123\",\n",
    "    \"g-126\",\n",
    "    \"g-128\",\n",
    "    \"g-131\",\n",
    "    \"g-132\",\n",
    "    \"g-134\",\n",
    "    \"g-135\",\n",
    "    \"g-138\",\n",
    "    \"g-139\",\n",
    "    \"g-140\",\n",
    "    \"g-142\",\n",
    "    \"g-144\",\n",
    "    \"g-145\",\n",
    "    \"g-146\",\n",
    "    \"g-147\",\n",
    "    \"g-148\",\n",
    "    \"g-152\",\n",
    "    \"g-155\",\n",
    "    \"g-157\",\n",
    "    \"g-158\",\n",
    "    \"g-160\",\n",
    "    \"g-163\",\n",
    "    \"g-164\",\n",
    "    \"g-165\",\n",
    "    \"g-170\",\n",
    "    \"g-173\",\n",
    "    \"g-174\",\n",
    "    \"g-175\",\n",
    "    \"g-177\",\n",
    "    \"g-178\",\n",
    "    \"g-181\",\n",
    "    \"g-183\",\n",
    "    \"g-185\",\n",
    "    \"g-186\",\n",
    "    \"g-189\",\n",
    "    \"g-192\",\n",
    "    \"g-194\",\n",
    "    \"g-195\",\n",
    "    \"g-196\",\n",
    "    \"g-197\",\n",
    "    \"g-199\",\n",
    "    \"g-201\",\n",
    "    \"g-202\",\n",
    "    \"g-206\",\n",
    "    \"g-208\",\n",
    "    \"g-210\",\n",
    "    \"g-213\",\n",
    "    \"g-214\",\n",
    "    \"g-215\",\n",
    "    \"g-220\",\n",
    "    \"g-226\",\n",
    "    \"g-228\",\n",
    "    \"g-229\",\n",
    "    \"g-235\",\n",
    "    \"g-238\",\n",
    "    \"g-241\",\n",
    "    \"g-242\",\n",
    "    \"g-243\",\n",
    "    \"g-244\",\n",
    "    \"g-245\",\n",
    "    \"g-248\",\n",
    "    \"g-250\",\n",
    "    \"g-251\",\n",
    "    \"g-254\",\n",
    "    \"g-257\",\n",
    "    \"g-259\",\n",
    "    \"g-261\",\n",
    "    \"g-266\",\n",
    "    \"g-270\",\n",
    "    \"g-271\",\n",
    "    \"g-272\",\n",
    "    \"g-275\",\n",
    "    \"g-278\",\n",
    "    \"g-282\",\n",
    "    \"g-287\",\n",
    "    \"g-288\",\n",
    "    \"g-289\",\n",
    "    \"g-291\",\n",
    "    \"g-293\",\n",
    "    \"g-294\",\n",
    "    \"g-297\",\n",
    "    \"g-298\",\n",
    "    \"g-301\",\n",
    "    \"g-303\",\n",
    "    \"g-304\",\n",
    "    \"g-306\",\n",
    "    \"g-308\",\n",
    "    \"g-309\",\n",
    "    \"g-310\",\n",
    "    \"g-311\",\n",
    "    \"g-314\",\n",
    "    \"g-315\",\n",
    "    \"g-316\",\n",
    "    \"g-317\",\n",
    "    \"g-320\",\n",
    "    \"g-321\",\n",
    "    \"g-322\",\n",
    "    \"g-327\",\n",
    "    \"g-328\",\n",
    "    \"g-329\",\n",
    "    \"g-332\",\n",
    "    \"g-334\",\n",
    "    \"g-335\",\n",
    "    \"g-336\",\n",
    "    \"g-337\",\n",
    "    \"g-339\",\n",
    "    \"g-342\",\n",
    "    \"g-344\",\n",
    "    \"g-349\",\n",
    "    \"g-350\",\n",
    "    \"g-351\",\n",
    "    \"g-353\",\n",
    "    \"g-354\",\n",
    "    \"g-355\",\n",
    "    \"g-357\",\n",
    "    \"g-359\",\n",
    "    \"g-360\",\n",
    "    \"g-364\",\n",
    "    \"g-365\",\n",
    "    \"g-366\",\n",
    "    \"g-367\",\n",
    "    \"g-368\",\n",
    "    \"g-369\",\n",
    "    \"g-374\",\n",
    "    \"g-375\",\n",
    "    \"g-377\",\n",
    "    \"g-379\",\n",
    "    \"g-385\",\n",
    "    \"g-386\",\n",
    "    \"g-390\",\n",
    "    \"g-392\",\n",
    "    \"g-393\",\n",
    "    \"g-400\",\n",
    "    \"g-402\",\n",
    "    \"g-406\",\n",
    "    \"g-407\",\n",
    "    \"g-409\",\n",
    "    \"g-410\",\n",
    "    \"g-411\",\n",
    "    \"g-414\",\n",
    "    \"g-417\",\n",
    "    \"g-418\",\n",
    "    \"g-421\",\n",
    "    \"g-423\",\n",
    "    \"g-424\",\n",
    "    \"g-427\",\n",
    "    \"g-429\",\n",
    "    \"g-431\",\n",
    "    \"g-432\",\n",
    "    \"g-433\",\n",
    "    \"g-434\",\n",
    "    \"g-437\",\n",
    "    \"g-439\",\n",
    "    \"g-440\",\n",
    "    \"g-443\",\n",
    "    \"g-449\",\n",
    "    \"g-458\",\n",
    "    \"g-459\",\n",
    "    \"g-460\",\n",
    "    \"g-461\",\n",
    "    \"g-464\",\n",
    "    \"g-467\",\n",
    "    \"g-468\",\n",
    "    \"g-470\",\n",
    "    \"g-473\",\n",
    "    \"g-477\",\n",
    "    \"g-478\",\n",
    "    \"g-479\",\n",
    "    \"g-484\",\n",
    "    \"g-485\",\n",
    "    \"g-486\",\n",
    "    \"g-488\",\n",
    "    \"g-489\",\n",
    "    \"g-491\",\n",
    "    \"g-494\",\n",
    "    \"g-496\",\n",
    "    \"g-498\",\n",
    "    \"g-500\",\n",
    "    \"g-503\",\n",
    "    \"g-504\",\n",
    "    \"g-506\",\n",
    "    \"g-508\",\n",
    "    \"g-509\",\n",
    "    \"g-512\",\n",
    "    \"g-522\",\n",
    "    \"g-529\",\n",
    "    \"g-531\",\n",
    "    \"g-534\",\n",
    "    \"g-539\",\n",
    "    \"g-541\",\n",
    "    \"g-546\",\n",
    "    \"g-551\",\n",
    "    \"g-553\",\n",
    "    \"g-554\",\n",
    "    \"g-559\",\n",
    "    \"g-561\",\n",
    "    \"g-562\",\n",
    "    \"g-565\",\n",
    "    \"g-568\",\n",
    "    \"g-569\",\n",
    "    \"g-574\",\n",
    "    \"g-577\",\n",
    "    \"g-578\",\n",
    "    \"g-586\",\n",
    "    \"g-588\",\n",
    "    \"g-590\",\n",
    "    \"g-594\",\n",
    "    \"g-595\",\n",
    "    \"g-596\",\n",
    "    \"g-597\",\n",
    "    \"g-599\",\n",
    "    \"g-600\",\n",
    "    \"g-603\",\n",
    "    \"g-607\",\n",
    "    \"g-615\",\n",
    "    \"g-618\",\n",
    "    \"g-619\",\n",
    "    \"g-620\",\n",
    "    \"g-625\",\n",
    "    \"g-628\",\n",
    "    \"g-629\",\n",
    "    \"g-632\",\n",
    "    \"g-634\",\n",
    "    \"g-635\",\n",
    "    \"g-636\",\n",
    "    \"g-638\",\n",
    "    \"g-639\",\n",
    "    \"g-641\",\n",
    "    \"g-643\",\n",
    "    \"g-644\",\n",
    "    \"g-645\",\n",
    "    \"g-646\",\n",
    "    \"g-647\",\n",
    "    \"g-648\",\n",
    "    \"g-663\",\n",
    "    \"g-664\",\n",
    "    \"g-665\",\n",
    "    \"g-668\",\n",
    "    \"g-669\",\n",
    "    \"g-670\",\n",
    "    \"g-671\",\n",
    "    \"g-672\",\n",
    "    \"g-673\",\n",
    "    \"g-674\",\n",
    "    \"g-677\",\n",
    "    \"g-678\",\n",
    "    \"g-680\",\n",
    "    \"g-683\",\n",
    "    \"g-689\",\n",
    "    \"g-691\",\n",
    "    \"g-693\",\n",
    "    \"g-695\",\n",
    "    \"g-701\",\n",
    "    \"g-702\",\n",
    "    \"g-703\",\n",
    "    \"g-704\",\n",
    "    \"g-705\",\n",
    "    \"g-706\",\n",
    "    \"g-708\",\n",
    "    \"g-711\",\n",
    "    \"g-712\",\n",
    "    \"g-720\",\n",
    "    \"g-721\",\n",
    "    \"g-723\",\n",
    "    \"g-724\",\n",
    "    \"g-726\",\n",
    "    \"g-728\",\n",
    "    \"g-731\",\n",
    "    \"g-733\",\n",
    "    \"g-738\",\n",
    "    \"g-739\",\n",
    "    \"g-742\",\n",
    "    \"g-743\",\n",
    "    \"g-744\",\n",
    "    \"g-745\",\n",
    "    \"g-749\",\n",
    "    \"g-750\",\n",
    "    \"g-752\",\n",
    "    \"g-760\",\n",
    "    \"g-761\",\n",
    "    \"g-764\",\n",
    "    \"g-766\",\n",
    "    \"g-768\",\n",
    "    \"g-770\",\n",
    "    \"g-771\",\n",
    "    \"c-0\",\n",
    "    \"c-1\",\n",
    "    \"c-2\",\n",
    "    \"c-3\",\n",
    "    \"c-4\",\n",
    "    \"c-5\",\n",
    "    \"c-6\",\n",
    "    \"c-7\",\n",
    "    \"c-8\",\n",
    "    \"c-9\",\n",
    "    \"c-10\",\n",
    "    \"c-11\",\n",
    "    \"c-12\",\n",
    "    \"c-13\",\n",
    "    \"c-14\",\n",
    "    \"c-15\",\n",
    "    \"c-16\",\n",
    "    \"c-17\",\n",
    "    \"c-18\",\n",
    "    \"c-19\",\n",
    "    \"c-20\",\n",
    "    \"c-21\",\n",
    "    \"c-22\",\n",
    "    \"c-23\",\n",
    "    \"c-24\",\n",
    "    \"c-25\",\n",
    "    \"c-26\",\n",
    "    \"c-27\",\n",
    "    \"c-28\",\n",
    "    \"c-29\",\n",
    "    \"c-30\",\n",
    "    \"c-31\",\n",
    "    \"c-32\",\n",
    "    \"c-33\",\n",
    "    \"c-34\",\n",
    "    \"c-35\",\n",
    "    \"c-36\",\n",
    "    \"c-37\",\n",
    "    \"c-38\",\n",
    "    \"c-39\",\n",
    "    \"c-40\",\n",
    "    \"c-41\",\n",
    "    \"c-42\",\n",
    "    \"c-43\",\n",
    "    \"c-44\",\n",
    "    \"c-45\",\n",
    "    \"c-46\",\n",
    "    \"c-47\",\n",
    "    \"c-48\",\n",
    "    \"c-49\",\n",
    "    \"c-50\",\n",
    "    \"c-51\",\n",
    "    \"c-52\",\n",
    "    \"c-53\",\n",
    "    \"c-54\",\n",
    "    \"c-55\",\n",
    "    \"c-56\",\n",
    "    \"c-57\",\n",
    "    \"c-58\",\n",
    "    \"c-59\",\n",
    "    \"c-60\",\n",
    "    \"c-61\",\n",
    "    \"c-62\",\n",
    "    \"c-63\",\n",
    "    \"c-64\",\n",
    "    \"c-65\",\n",
    "    \"c-66\",\n",
    "    \"c-67\",\n",
    "    \"c-68\",\n",
    "    \"c-69\",\n",
    "    \"c-70\",\n",
    "    \"c-71\",\n",
    "    \"c-72\",\n",
    "    \"c-73\",\n",
    "    \"c-74\",\n",
    "    \"c-75\",\n",
    "    \"c-76\",\n",
    "    \"c-77\",\n",
    "    \"c-78\",\n",
    "    \"c-79\",\n",
    "    \"c-80\",\n",
    "    \"c-81\",\n",
    "    \"c-82\",\n",
    "    \"c-83\",\n",
    "    \"c-84\",\n",
    "    \"c-85\",\n",
    "    \"c-86\",\n",
    "    \"c-87\",\n",
    "    \"c-88\",\n",
    "    \"c-89\",\n",
    "    \"c-90\",\n",
    "    \"c-91\",\n",
    "    \"c-92\",\n",
    "    \"c-93\",\n",
    "    \"c-94\",\n",
    "    \"c-95\",\n",
    "    \"c-96\",\n",
    "    \"c-97\",\n",
    "    \"c-98\",\n",
    "    \"c-99\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T06:00:26.460079Z",
     "start_time": "2020-11-29T06:00:26.450106Z"
    },
    "code_folding": [],
    "execution": {
     "iopub.execute_input": "2020-11-15T04:40:27.492218Z",
     "iopub.status.busy": "2020-11-15T04:40:27.491480Z",
     "iopub.status.idle": "2020-11-15T10:01:54.657676Z",
     "shell.execute_reply": "2020-11-15T10:01:54.658434Z"
    },
    "papermill": {
     "duration": 19287.2044,
     "end_time": "2020-11-15T10:01:54.658768",
     "exception": false,
     "start_time": "2020-11-15T04:40:27.454368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(batch_size, model_type=\"rs\", params={}):\n",
    "    counts = np.empty((n_seeds * n_splits, n_classes))\n",
    "\n",
    "    bias_initializer = -Y.mean(axis=0).apply(np.log).values\n",
    "    bias_initializer = tf.keras.initializers.Constant(bias_initializer)\n",
    "\n",
    "    Y_pred = np.zeros((train_size, n_classes))\n",
    "    Y_pred = pd.DataFrame(Y_pred, columns=Y.columns, index=Y.index)\n",
    "\n",
    "    for i in range(n_seeds):\n",
    "    #for i in [1,2,3,4]:  # gpu死んだので途中から再実行\n",
    "    #for i in [3,4]:  # gpu死んだので途中から再実行\n",
    "        print(f\"\\n---------- seed: {i} ----------\")\n",
    "        set_seed(seed=i)\n",
    "\n",
    "        cv = MultilabelStratifiedGroupKFold(\n",
    "            n_splits=n_splits, random_state=i, shuffle=shuffle\n",
    "        )\n",
    "\n",
    "        for j, (train, valid) in enumerate(cv.split(X, Y[columns], groups)):\n",
    "            print(f\"\\n================= fold: {j} =================\")\n",
    "            counts[i * n_splits + j] = Y.iloc[train].sum()\n",
    "\n",
    "            os.makedirs(model_type, exist_ok=True)\n",
    "\n",
    "            # model_nonscored_path = f\"model_nonscored_seed_{i}_fold_{j}.h5\"\n",
    "            model_path = f\"{model_type}/model_seed_{i}_fold_{j}.h5\"\n",
    "\n",
    "            K.clear_session()\n",
    "            if model_type == \"5l\":\n",
    "                model = create_model_5l(n_features, num_classes=n_classes, **params)\n",
    "            elif model_type == \"4l\":\n",
    "                model = create_model_4l(n_features, num_classes=n_classes, **params)\n",
    "            elif model_type == \"3l_v2\":\n",
    "                model = create_model_3l_v2(n_features, num_classes=n_classes, **params)\n",
    "            elif model_type == \"2l\":\n",
    "                model = create_model_2l(n_features, num_classes=n_classes, **params)\n",
    "            elif model_type == \"rs\":\n",
    "                model = create_model_rs(\n",
    "                    n_features, len(start_predictors), num_classes=n_classes, **params\n",
    "                )\n",
    "            elif model_type == \"StackedTabNet\":\n",
    "                model = create_model_stacked_tabnet(n_features, num_classes=n_classes)\n",
    "\n",
    "            if model_type == \"rs\":\n",
    "                # 入力2つのNN使うから工夫してる\n",
    "                X_ = X[start_predictors]\n",
    "\n",
    "                callbacks = build_callbacks(\n",
    "                    model_path, factor=factor, patience=patience\n",
    "                )\n",
    "                history = model.fit(\n",
    "                    [X.iloc[train], X_.iloc[train]],\n",
    "                    Y.iloc[train],\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=([X.iloc[valid], X_.iloc[valid]], Y.iloc[valid]),\n",
    "                    **fit_params,\n",
    "                )\n",
    "\n",
    "                model.load_weights(model_path)\n",
    "\n",
    "                Y_pred.iloc[valid] += (\n",
    "                    model.predict([X.iloc[valid], X_.iloc[valid]]) / n_seeds\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                generator = Cutmix(\n",
    "                    X.iloc[train], Y.iloc[train], alpha=alpha, batch_size=batch_size\n",
    "                )\n",
    "                callbacks = build_callbacks(\n",
    "                    model_path, factor=factor, patience=patience\n",
    "                )\n",
    "#                history = model.fit(\n",
    "#                    generator,\n",
    "#                    callbacks=callbacks,\n",
    "#                    validation_data=(X.iloc[valid], Y.iloc[valid]),\n",
    "#                    **fit_params,\n",
    "#                )\n",
    "                if model_type == \"StackedTabNet\":\n",
    "                    model(np.zeros((1, n_features)))\n",
    "\n",
    "                model.load_weights(model_path)\n",
    "\n",
    "                Y_pred.iloc[valid] += model.predict(X.iloc[valid]) / n_seeds\n",
    "                \n",
    "            del model\n",
    "            gc.collect()\n",
    "\n",
    "    Y_pred[train_features[\"cp_type\"] == \"ctl_vehicle\"] = 0.0\n",
    "\n",
    "    with open(\"counts.pkl\", \"wb\") as f:\n",
    "        pickle.dump(counts, f)\n",
    "\n",
    "    with open(f\"Y_pred_{model_type}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(Y_pred[columns], f)\n",
    "\n",
    "    oof_score = score(Y[columns], Y_pred[columns])\n",
    "    print(f\"oof_score: {oof_score}\")\n",
    "\n",
    "    oof_auc_score = auc_score(Y[columns], Y_pred[columns])\n",
    "    print(f\"oof_auc_score: {oof_auc_score}\")\n",
    "\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    return oof_score, Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T14:20:31.403686Z",
     "start_time": "2020-11-28T11:59:54.727598Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0429s vs `on_train_batch_end` time: 0.1423s). Check your callbacks.\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0598s vs `on_train_batch_end` time: 0.1586s). Check your callbacks.\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0449s vs `on_train_batch_end` time: 0.1316s). Check your callbacks.\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0479s vs `on_train_batch_end` time: 0.1257s). Check your callbacks.\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0628s vs `on_train_batch_end` time: 0.1346s). Check your callbacks.\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass shuffle=True, random_state=1 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0667s vs `on_train_batch_end` time: 0.2834s). Check your callbacks.\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "GPU sync failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-091eb23bbe96>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(batch_size, model_type, params)\u001b[0m\n\u001b[0;32m     68\u001b[0m                     \u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfactor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpatience\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                 )\n\u001b[1;32m---> 70\u001b[1;33m                 history = model.fit(\n\u001b[0m\u001b[0;32m     71\u001b[0m                     \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1101\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[1;31m# No error, now safe to assign to logs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1103\u001b[1;33m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1104\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \"\"\"\n\u001b[0;32m    439\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    287\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unrecognized hook: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    307\u001b[0m       \u001b[0mbatch_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    343\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Only convert once.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m           \u001b[0mnumpy_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 635\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 635\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    531\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m     \"\"\"\n\u001b[0;32m   1062\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1029\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1031\u001b[1;33m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1032\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: GPU sync failed"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if __name__ == \"__main__\":\n",
    "    # MLPs\n",
    "    #oof_score, Y_pred = train_and_evaluate(model_type=\"5l\", batch_size=32)\n",
    "    #oof_score, Y_pred = train_and_evaluate(model_type=\"4l\", batch_size=32)\n",
    "    #oof_score, Y_pred = train_and_evaluate(model_type=\"3l_v2\", batch_size=32)\n",
    "    #oof_score, Y_pred = train_and_evaluate(model_type=\"2l\", batch_size=32)\n",
    "    #oof_score, Y_pred = train_and_evaluate(model_type=\"rs\", batch_size=32)\n",
    "\n",
    "    # Stacked TabNet\n",
    "    # oof_score, Y_pred = train_and_evaluate(model_type=\"StackedTabNet\", batch_size=8)  # batch_size=8 は1fold4時間以上かかるからやめる\n",
    "    oof_score, Y_pred = train_and_evaluate(model_type=\"StackedTabNet\", batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-28T14:39:32.421Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- seed: 1 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass shuffle=True, random_state=1 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= fold: 0 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0319s vs `on_train_batch_end` time: 0.1168s). Check your callbacks.\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 1 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0459s vs `on_train_batch_end` time: 0.1247s). Check your callbacks.\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 2 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0499s vs `on_train_batch_end` time: 0.1277s). Check your callbacks.\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 3 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0379s vs `on_train_batch_end` time: 0.1147s). Check your callbacks.\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 4 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0309s vs `on_train_batch_end` time: 0.1307s). Check your callbacks.\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "---------- seed: 2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass shuffle=True, random_state=2 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= fold: 0 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0362s vs `on_train_batch_end` time: 0.0863s). Check your callbacks.\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 1 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0299s vs `on_train_batch_end` time: 0.0968s). Check your callbacks.\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 2 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0459s vs `on_train_batch_end` time: 0.1006s). Check your callbacks.\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 3 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0269s vs `on_train_batch_end` time: 0.0720s). Check your callbacks.\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 4 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0383s vs `on_train_batch_end` time: 0.1047s). Check your callbacks.\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "---------- seed: 3 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass shuffle=True, random_state=3 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= fold: 0 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0439s vs `on_train_batch_end` time: 0.0977s). Check your callbacks.\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# gpu死んだので途中から再実行\n",
    "if __name__ == \"__main__\":\n",
    "    oof_score, Y_pred = train_and_evaluate(model_type=\"StackedTabNet\", batch_size=128)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T05:54:18.997435Z",
     "start_time": "2020-11-29T03:49:57.360259Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- seed: 3 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass shuffle=True, random_state=3 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= fold: 0 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0199s vs `on_train_batch_end` time: 0.0688s). Check your callbacks.\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 1 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0250s vs `on_train_batch_end` time: 0.0608s). Check your callbacks.\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 2 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0279s vs `on_train_batch_end` time: 0.0658s). Check your callbacks.\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 3 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0269s vs `on_train_batch_end` time: 0.0648s). Check your callbacks.\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 4 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0269s vs `on_train_batch_end` time: 0.0738s). Check your callbacks.\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "---------- seed: 4 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass shuffle=True, random_state=4 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= fold: 0 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0260s vs `on_train_batch_end` time: 0.0728s). Check your callbacks.\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 1 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0219s vs `on_train_batch_end` time: 0.0699s). Check your callbacks.\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 2 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0240s vs `on_train_batch_end` time: 0.0638s). Check your callbacks.\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 3 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0160s vs `on_train_batch_end` time: 0.0409s). Check your callbacks.\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 4 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0259s vs `on_train_batch_end` time: 0.0698s). Check your callbacks.\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "oof_score: 0.017082749448390062\n",
      "oof_auc_score: 0.6884487207078382\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Wall time: 2h 4min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# gpu死んだので途中から再実行\n",
    "if __name__ == \"__main__\":\n",
    "    oof_score, Y_pred = train_and_evaluate(model_type=\"StackedTabNet\", batch_size=128)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T06:01:17.116570Z",
     "start_time": "2020-11-29T06:00:31.901853Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- seed: 0 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= fold: 0 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 1 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 2 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 3 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 4 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "---------- seed: 1 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass shuffle=True, random_state=1 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= fold: 0 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 1 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 2 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 3 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 4 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "---------- seed: 2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass shuffle=True, random_state=2 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= fold: 0 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 1 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 2 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 3 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 4 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "---------- seed: 3 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass shuffle=True, random_state=3 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= fold: 0 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 1 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 2 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 3 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 4 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "---------- seed: 4 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\81908\\appdata\\local\\pypoetry\\cache\\virtualenvs\\tfgpu-ehdmne1y-py3.8\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass shuffle=True, random_state=4 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= fold: 0 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 1 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 2 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 3 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "\n",
      "================= fold: 4 =================\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      "Current version (0.1.0)  1e-14  supported          default: True\n",
      "\u001b[31mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[31mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[0m\n",
      "WARNING:tensorflow:Layer stacked_tab_net_classifier is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "oof_score: 0.015609382377012933\n",
      "oof_auc_score: 0.6854744842777921\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Wall time: 45.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 学習処理コメントアウトしてoof確認\n",
    "if __name__ == \"__main__\":\n",
    "    oof_score, Y_pred = train_and_evaluate(model_type=\"StackedTabNet\", batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.6.0"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "papermill": {
   "duration": 19308.710664,
   "end_time": "2020-11-15T10:01:56.008945",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-15T04:40:07.298281",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
