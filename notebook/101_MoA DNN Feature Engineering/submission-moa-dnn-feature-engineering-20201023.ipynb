{"cells":[{"metadata":{},"cell_type":"markdown","source":"参考: https://www.kaggle.com/ragnar123/moa-dnn-feature-engineering\n\n\n特徴量選択 + 統計量の特徴量追加 + pcaで圧縮した特徴量追加 + skip connectionなど使ったMLPモデルブレンディング + label-smoothing + seed avg"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import sys\n\nsys.path.append('../input/iterativestratification')\n#sys.path.append(r\"C:\\Users\\81908\\Git\\iterative-stratification\")\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow_addons as tfa\nfrom sklearn.metrics import log_loss\nimport random\nimport os\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"FOLDS = 10  # cvの数\nLR = 0.001\n\n# Seed for deterministic results\nSEEDS1 = [1, 2, 3, 4, 5, 6, 7]\nSEEDS2 = [8, 9, 10, 11, 12, 13, 14]\nSEEDS3 = [15, 16, 17, 18, 19, 20, 21]\nSEEDS4 = [22, 23, 24, 25, 26, 27, 28]\nSEEDS5 = [29, 30, 31, 32, 33, 34, 35]\n\n# Function to seed everything\ndef seed_everything(seed):\n    \"\"\"乱数固定\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    tf.random.set_seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to map an filter out control group\ndef mapping_and_filter(train, train_targets, test):\n    \"\"\"前処理\"\"\"\n    cp_type = {\"trt_cp\": 0, \"ctl_vehicle\": 1}\n    cp_dose = {\"D1\": 0, \"D2\": 1}\n    for df in [train, test]:\n        df[\"cp_type\"] = df[\"cp_type\"].map(cp_type)\n        df[\"cp_dose\"] = df[\"cp_dose\"].map(cp_dose)\n    # ctl_vehicleは必ず0なので学習データから除く\n    train_targets = train_targets[train[\"cp_type\"] == 0].reset_index(drop=True)\n    train = train[train[\"cp_type\"] == 0].reset_index(drop=True)\n    # sig_id列はidなので不要\n    train_targets.drop([\"sig_id\"], inplace=True, axis=1)\n    return train, train_targets, test\n\n\n# Function to scale our data\ndef scaling(train, test):\n    \"\"\"規格化。pcaの後に実行してる。pcaの後だから外れ値にロバストな規格化使ってるみたい\"\"\"\n    features = train.columns[2:]\n    scaler = RobustScaler()  # 外れ値に頑健な標準化\n    scaler.fit(pd.concat([train[features], test[features]], axis=0))\n    train[features] = scaler.transform(train[features])\n    test[features] = scaler.transform(test[features])\n    return train, test, features\n\n\n# Function to extract pca features\ndef fe_pca(train, test, n_components_g=70, n_components_c=10, SEED=123):\n    \"\"\"pcaで特徴量圧縮\"\"\"\n\n    # 特徴量分けているが大区分がgとcの2区分あるので、それぞれでpca\n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n\n    def create_pca(train, test, features, kind=\"g\", n_components=n_components_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis=0)\n        pca = PCA(n_components=n_components, random_state=SEED)\n        data = pca.fit_transform(data)\n        columns = [f\"pca_{kind}{i + 1}\" for i in range(n_components)]\n        data = pd.DataFrame(data, columns=columns)\n        train_ = data.iloc[: train.shape[0]]\n        test_ = data.iloc[train.shape[0] :].reset_index(drop=True)\n        train = pd.concat([train, train_], axis=1)\n        test = pd.concat([test, test_], axis=1)\n        return train, test\n\n    train, test = create_pca(\n        train, test, features_g, kind=\"g\", n_components=n_components_g\n    )\n    train, test = create_pca(\n        train, test, features_c, kind=\"c\", n_components=n_components_c\n    )\n    return train, test\n\n\n# Function to extract common stats features\ndef fe_stats(train, test):\n    \"\"\"統計量の特徴量追加\"\"\"\n\n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n\n    for df in [train, test]:\n        df[\"g_sum\"] = df[features_g].sum(axis=1)\n        df[\"g_mean\"] = df[features_g].mean(axis=1)\n        df[\"g_std\"] = df[features_g].std(axis=1)\n        df[\"g_kurt\"] = df[features_g].kurtosis(axis=1)\n        df[\"g_skew\"] = df[features_g].skew(axis=1)\n        df[\"c_sum\"] = df[features_c].sum(axis=1)\n        df[\"c_mean\"] = df[features_c].mean(axis=1)\n        df[\"c_std\"] = df[features_c].std(axis=1)\n        df[\"c_kurt\"] = df[features_c].kurtosis(axis=1)\n        df[\"c_skew\"] = df[features_c].skew(axis=1)\n        df[\"gc_sum\"] = df[features_g + features_c].sum(axis=1)\n        df[\"gc_mean\"] = df[features_g + features_c].mean(axis=1)\n        df[\"gc_std\"] = df[features_g + features_c].std(axis=1)\n        df[\"gc_kurt\"] = df[features_g + features_c].kurtosis(axis=1)\n        df[\"gc_skew\"] = df[features_g + features_c].skew(axis=1)\n\n    return train, test\n\n\ndef c_squared(train, test):\n    \"\"\"cの特徴量について2乗した特徴量作成\"\"\"\n\n    features_c = list(train.columns[776:876])\n    for df in [train, test]:\n        for feature in features_c:\n            df[f\"{feature}_squared\"] = df[feature] ** 2\n    return train, test\n\n\n# Function to calculate the mean log loss of the targets including clipping\ndef mean_log_loss(y_true, y_pred):\n    \"\"\"マルチラベル全体でlog lossを平均する\"\"\"\n\n    # 評価指標がlog losだからか？+label smoothingするため、予測ラベルはクリッピングする\n    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    metrics = []\n    for target in range(206):\n        metrics.append(log_loss(y_true[:, target], y_pred[:, target]))\n    return np.mean(metrics)\n\n\ndef create_model_rs(shape1, shape2):\n    \"\"\"入力2つのNN.resnetみたくskip connection入れてる\"\"\"\n    input_1 = tf.keras.layers.Input(shape=(shape1))\n    input_2 = tf.keras.layers.Input(shape=(shape2))\n\n    head_1 = tf.keras.layers.BatchNormalization()(input_1)\n    head_1 = tf.keras.layers.Dropout(0.2)(head_1)\n    head_1 = tf.keras.layers.Dense(512, activation=\"elu\")(head_1)\n    head_1 = tf.keras.layers.BatchNormalization()(head_1)\n    input_3 = tf.keras.layers.Dense(256, activation=\"elu\")(head_1)\n\n    input_3_concat = tf.keras.layers.Concatenate()(\n        [input_2, input_3]\n    )  # node連結。node数が2つのnodeの足し算になる\n\n    head_2 = tf.keras.layers.BatchNormalization()(input_3_concat)\n    head_2 = tf.keras.layers.Dropout(0.3)(head_2)\n    head_2 = tf.keras.layers.Dense(512, \"relu\")(head_2)\n    head_2 = tf.keras.layers.BatchNormalization()(head_2)\n    head_2 = tf.keras.layers.Dense(512, \"elu\")(head_2)\n    head_2 = tf.keras.layers.BatchNormalization()(head_2)\n    head_2 = tf.keras.layers.Dense(256, \"relu\")(head_2)\n    head_2 = tf.keras.layers.BatchNormalization()(head_2)\n    input_4 = tf.keras.layers.Dense(256, \"elu\")(head_2)\n\n    input_4_avg = tf.keras.layers.Average()([input_3, input_4])  # 入力のリストを要素ごとに平均化\n\n    head_3 = tf.keras.layers.BatchNormalization()(input_4_avg)\n    head_3 = tf.keras.layers.Dense(\n        256, kernel_initializer=\"lecun_normal\", activation=\"selu\"\n    )(head_3)\n    head_3 = tf.keras.layers.BatchNormalization()(head_3)\n    head_3 = tf.keras.layers.Dense(\n        206, kernel_initializer=\"lecun_normal\", activation=\"selu\"\n    )(head_3)\n    head_3 = tf.keras.layers.BatchNormalization()(head_3)\n    output = tf.keras.layers.Dense(206, activation=\"sigmoid\")(head_3)\n\n    model = tf.keras.models.Model(inputs=[input_1, input_2], outputs=output)\n    opt = tf.optimizers.Adam(learning_rate=LR)\n    model.compile(\n        optimizer=opt,\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.0015),  # ラベルスムージング\n        metrics=tf.keras.metrics.BinaryCrossentropy(),\n    )\n\n    return model\n\n\n# Function to create our 5 layer dnn model\ndef create_model_5l(shape):\n    \"\"\"入力1つの5層NN。Stochastic Weight Averaging使う\"\"\"\n    inp = tf.keras.layers.Input(shape=(shape))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    x = tf.keras.layers.Dense(2560, activation=\"relu\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    x = tf.keras.layers.Dense(2048, activation=\"relu\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    x = tf.keras.layers.Dense(1524, activation=\"relu\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    x = tf.keras.layers.Dense(1012, activation=\"relu\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    x = tf.keras.layers.Dense(780, activation=\"relu\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    out = tf.keras.layers.Dense(206, activation=\"sigmoid\")(x)\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    opt = tf.optimizers.Adam(learning_rate=LR)\n    opt = tfa.optimizers.SWA(\n        opt\n    )  # Stochastic Weight Averaging.モデルの重みを、これまで＋今回の平均を取って更新していくことでうまくいくみたい https://twitter.com/icoxfog417/status/989762534163992577\n    model.compile(\n        optimizer=opt,\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.0020),  # ラベルスムージング\n        metrics=tf.keras.metrics.BinaryCrossentropy(),\n    )\n    return model\n\n\n# Function to create our 4 layer dnn model\ndef create_model_4l(shape):\n    \"\"\"入力1つの4層NN。シンプル\"\"\"\n    inp = tf.keras.layers.Input(shape=(shape))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    x = tf.keras.layers.Dense(2048, activation=\"relu\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    x = tf.keras.layers.Dense(1524, activation=\"relu\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    x = tf.keras.layers.Dense(1012, activation=\"relu\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    x = tf.keras.layers.Dense(1012, activation=\"relu\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    out = tf.keras.layers.Dense(206, activation=\"sigmoid\")(x)\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    opt = tf.optimizers.Adam(learning_rate=LR)\n    model.compile(\n        optimizer=opt,\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.0020),  # ラベルスムージング\n        metrics=tf.keras.metrics.BinaryCrossentropy(),\n    )\n    return model\n\n\n# Function to create our 3 layer dnn model\ndef create_model_3l(shape):\n    \"\"\"入力1つの3層NN。Lookahead, WeightNormalization使う\"\"\"\n    inp = tf.keras.layers.Input(shape=(shape))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(0.4914099166744246)(x)\n    x = tfa.layers.WeightNormalization(tf.keras.layers.Dense(1159, activation=\"relu\"))(\n        x\n    )\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.18817607797795838)(x)\n    x = tfa.layers.WeightNormalization(tf.keras.layers.Dense(960, activation=\"relu\"))(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.12542057776853896)(x)\n    x = tfa.layers.WeightNormalization(tf.keras.layers.Dense(1811, activation=\"relu\"))(\n        x\n    )\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.20175242230280122)(x)\n    out = tfa.layers.WeightNormalization(\n        tf.keras.layers.Dense(206, activation=\"sigmoid\")\n    )(x)\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    opt = tf.optimizers.Adam(learning_rate=LR)\n    opt = tfa.optimizers.Lookahead(\n        opt, sync_period=10\n    )  # Lookahead.勾配の更新方法を工夫してるみたい。学習率の設定にロバストが手法 https://cyberagent.ai/blog/research/11410/\n    model.compile(\n        optimizer=opt,\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.0015),\n        metrics=tf.keras.metrics.BinaryCrossentropy(),\n    )\n    return model\n\n\n# Function to create our 2 layer dnn model\ndef create_model_2l(shape):\n    \"\"\"入力1つの2層NN。Lookahead, WeightNormalization使う\"\"\"\n    inp = tf.keras.layers.Input(shape=(shape))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(0.2688628097505064)(x)\n    x = tfa.layers.WeightNormalization(tf.keras.layers.Dense(1292, activation=\"relu\"))(\n        x\n    )\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.4598218403250696)(x)\n    x = tfa.layers.WeightNormalization(tf.keras.layers.Dense(983, activation=\"relu\"))(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.4703144018483698)(x)\n    out = tfa.layers.WeightNormalization(\n        tf.keras.layers.Dense(206, activation=\"sigmoid\")\n    )(x)\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    opt = tf.optimizers.Adam(learning_rate=LR)\n    opt = tfa.optimizers.Lookahead(opt, sync_period=10)\n    model.compile(\n        optimizer=opt,\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.0015),\n        metrics=tf.keras.metrics.BinaryCrossentropy(),\n    )\n    return model\n\n\n# Function to train our dnn\ndef inference(\n    train,\n    test,\n    train_targets,\n    features,\n    start_predictors,\n    SEED=123,\n    MODEL=\"3l\",\n    PATH=\"../input/moa-3layer\",\n):\n    \"\"\"推論用\"\"\"\n    seed_everything(SEED)\n    oof_pred = np.zeros((train.shape[0], 206))\n    test_pred = np.zeros((test.shape[0], 206))\n    for fold, (trn_ind, val_ind) in enumerate(\n        MultilabelStratifiedKFold(\n            n_splits=FOLDS, random_state=SEED, shuffle=True\n        ).split(train_targets, train_targets)\n    ):\n        K.clear_session()\n        if MODEL == \"5l\":\n            model = create_model_5l(len(features))\n        elif MODEL == \"4l\":\n            model = create_model_4l(len(features))\n        elif MODEL == \"3l\":\n            model = create_model_3l(len(features))\n        elif MODEL == \"2l\":\n            model = create_model_2l(len(features))\n        elif MODEL == \"rs\":\n            model = create_model_rs(len(features), len(start_predictors))\n\n        x_train, x_val = (\n            train[features].values[trn_ind],\n            train[features].values[val_ind],\n        )\n        y_train, y_val = train_targets.values[trn_ind], train_targets.values[val_ind]\n\n        model.load_weights(f\"{PATH}/{MODEL}_{fold}_{SEED}.h5\")\n\n        if MODEL == \"rs\":\n            x_train_, x_val_ = (\n                train[start_predictors].values[trn_ind],\n                train[start_predictors].values[val_ind],\n            )\n            oof_pred[val_ind] = model.predict([x_val, x_val_])\n            test_pred += (\n                model.predict([test[features].values, test[start_predictors].values])\n                / FOLDS\n            )\n        else:\n            oof_pred[val_ind] = model.predict(x_val)\n            test_pred += model.predict(test[features].values) / FOLDS\n\n    oof_score = mean_log_loss(train_targets.values, oof_pred)\n    print(f\"Our out of folds mean log loss score is {oof_score}\")\n\n    return test_pred, oof_pred\n\n\n# Function to train our model with multiple seeds and average the predictions\ndef run_multiple_seeds(\n    train,\n    test,\n    train_targets,\n    features,\n    start_predictors,\n    SEEDS=[123],\n    MODEL=\"3l\",\n    PATH=\"../input/moa-3layer\",\n):\n    \"\"\"cvのseed変えて推論をseed アンサンブル\"\"\"\n    test_pred = []\n    oof_pred = []\n\n    for SEED in SEEDS:\n        print(f\"Using model {MODEL} with seed {SEED} for inference\")\n        print(f\"Trained with {len(features)} features\")\n        test_pred_, oof_pred_ = inference(\n            train,\n            test,\n            train_targets,\n            features,\n            start_predictors,\n            SEED=SEED,\n            MODEL=MODEL,\n            PATH=PATH,\n        )\n        test_pred.append(test_pred_)\n        oof_pred.append(oof_pred_)\n        print(\"-\" * 50)\n        print(\"\\n\")\n\n    test_pred = np.average(test_pred, axis=0)\n    oof_pred = np.average(oof_pred, axis=0)\n\n    seed_log_loss = mean_log_loss(train_targets.values, oof_pred)\n    print(f\"Our out of folds log loss for our seed blend model is {seed_log_loss}\")\n\n    return test_pred, oof_pred\n\n\ndef submission(test_pred):\n    sample_submission.loc[:, train_targets.columns] = test_pred\n    sample_submission.loc[test[\"cp_type\"] == 1, train_targets.columns] = 0\n    sample_submission.to_csv(\"submission.csv\", index=False)\n    return sample_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_submission(model_dir=\"../input\"):\n    \"\"\"モデル推論してsubmissioファイル作成\"\"\"\n    # Inference time\n    test_pred_5l, oof_pred_5l = run_multiple_seeds(\n        train,\n        test,\n        train_targets,\n        features,\n        start_predictors,\n        SEEDS=SEEDS1,\n        MODEL=\"5l\",\n        PATH=f\"{model_dir}/moa-5layer\",\n    )\n    test_pred_4l, oof_pred_4l = run_multiple_seeds(\n        train,\n        test,\n        train_targets,\n        features,\n        start_predictors,\n        SEEDS=SEEDS2,\n        MODEL=\"4l\",\n        PATH=f\"{model_dir}/moa-4layer\",\n    )\n    test_pred_3l, oof_pred_3l = run_multiple_seeds(\n        train,\n        test,\n        train_targets,\n        features,\n        start_predictors,\n        SEEDS=SEEDS3,\n        MODEL=\"3l\",\n        PATH=f\"{model_dir}/moa-3layer\",\n    )\n    test_pred_2l, oof_pred_2l = run_multiple_seeds(\n        train,\n        test,\n        train_targets,\n        features,\n        start_predictors,\n        SEEDS=SEEDS4,\n        MODEL=\"2l\",\n        PATH=f\"{model_dir}/moa-2layer\",\n    )\n    test_pred_rs, oof_pred_rs = run_multiple_seeds(\n        train,\n        test,\n        train_targets,\n        features,\n        start_predictors,\n        SEEDS=SEEDS5,\n        MODEL=\"rs\",\n        PATH=f\"{model_dir}/moa-rs\",\n    )\n\n    # Blend 5l, 4l, 3l and l2 dnn model\n    oof_pred = np.average([oof_pred_5l, oof_pred_4l, oof_pred_3l, oof_pred_2l], axis=0)\n    seed_log_loss = mean_log_loss(train_targets.values, oof_pred)\n    print(\n        f\"Our final out of folds log loss for our classic dnn blend is {seed_log_loss}\"\n    )\n    test_pred = np.average(\n        [test_pred_5l, test_pred_4l, test_pred_3l, test_pred_2l], axis=0\n    )\n\n    # Blend the result of the previous model with the dnn resnet type model\n    oof_pred = np.average([oof_pred, oof_pred_rs], axis=0)\n    seed_log_loss = mean_log_loss(train_targets.values, oof_pred)\n    print(\n        f\"Our final out of folds log loss for our classic dnn + dnn resnet type model is {seed_log_loss}\"\n    )\n    test_pred = np.average([test_pred, test_pred_rs], axis=0)\n\n    sample_submission = submission(test_pred)\n    # sample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Got this predictors from public kernels for the resnet type model\n# 特徴量選択しておくみたい public kerelでやってるのパクリらしい\nstart_predictors = [\n    \"g-0\",\n    \"g-7\",\n    \"g-8\",\n    \"g-10\",\n    \"g-13\",\n    \"g-17\",\n    \"g-20\",\n    \"g-22\",\n    \"g-24\",\n    \"g-26\",\n    \"g-28\",\n    \"g-29\",\n    \"g-30\",\n    \"g-31\",\n    \"g-32\",\n    \"g-34\",\n    \"g-35\",\n    \"g-36\",\n    \"g-37\",\n    \"g-38\",\n    \"g-39\",\n    \"g-41\",\n    \"g-46\",\n    \"g-48\",\n    \"g-50\",\n    \"g-51\",\n    \"g-52\",\n    \"g-55\",\n    \"g-58\",\n    \"g-59\",\n    \"g-61\",\n    \"g-62\",\n    \"g-63\",\n    \"g-65\",\n    \"g-66\",\n    \"g-67\",\n    \"g-68\",\n    \"g-70\",\n    \"g-72\",\n    \"g-74\",\n    \"g-75\",\n    \"g-79\",\n    \"g-83\",\n    \"g-84\",\n    \"g-85\",\n    \"g-86\",\n    \"g-90\",\n    \"g-91\",\n    \"g-94\",\n    \"g-95\",\n    \"g-96\",\n    \"g-97\",\n    \"g-98\",\n    \"g-100\",\n    \"g-102\",\n    \"g-105\",\n    \"g-106\",\n    \"g-112\",\n    \"g-113\",\n    \"g-114\",\n    \"g-116\",\n    \"g-121\",\n    \"g-123\",\n    \"g-126\",\n    \"g-128\",\n    \"g-131\",\n    \"g-132\",\n    \"g-134\",\n    \"g-135\",\n    \"g-138\",\n    \"g-139\",\n    \"g-140\",\n    \"g-142\",\n    \"g-144\",\n    \"g-145\",\n    \"g-146\",\n    \"g-147\",\n    \"g-148\",\n    \"g-152\",\n    \"g-155\",\n    \"g-157\",\n    \"g-158\",\n    \"g-160\",\n    \"g-163\",\n    \"g-164\",\n    \"g-165\",\n    \"g-170\",\n    \"g-173\",\n    \"g-174\",\n    \"g-175\",\n    \"g-177\",\n    \"g-178\",\n    \"g-181\",\n    \"g-183\",\n    \"g-185\",\n    \"g-186\",\n    \"g-189\",\n    \"g-192\",\n    \"g-194\",\n    \"g-195\",\n    \"g-196\",\n    \"g-197\",\n    \"g-199\",\n    \"g-201\",\n    \"g-202\",\n    \"g-206\",\n    \"g-208\",\n    \"g-210\",\n    \"g-213\",\n    \"g-214\",\n    \"g-215\",\n    \"g-220\",\n    \"g-226\",\n    \"g-228\",\n    \"g-229\",\n    \"g-235\",\n    \"g-238\",\n    \"g-241\",\n    \"g-242\",\n    \"g-243\",\n    \"g-244\",\n    \"g-245\",\n    \"g-248\",\n    \"g-250\",\n    \"g-251\",\n    \"g-254\",\n    \"g-257\",\n    \"g-259\",\n    \"g-261\",\n    \"g-266\",\n    \"g-270\",\n    \"g-271\",\n    \"g-272\",\n    \"g-275\",\n    \"g-278\",\n    \"g-282\",\n    \"g-287\",\n    \"g-288\",\n    \"g-289\",\n    \"g-291\",\n    \"g-293\",\n    \"g-294\",\n    \"g-297\",\n    \"g-298\",\n    \"g-301\",\n    \"g-303\",\n    \"g-304\",\n    \"g-306\",\n    \"g-308\",\n    \"g-309\",\n    \"g-310\",\n    \"g-311\",\n    \"g-314\",\n    \"g-315\",\n    \"g-316\",\n    \"g-317\",\n    \"g-320\",\n    \"g-321\",\n    \"g-322\",\n    \"g-327\",\n    \"g-328\",\n    \"g-329\",\n    \"g-332\",\n    \"g-334\",\n    \"g-335\",\n    \"g-336\",\n    \"g-337\",\n    \"g-339\",\n    \"g-342\",\n    \"g-344\",\n    \"g-349\",\n    \"g-350\",\n    \"g-351\",\n    \"g-353\",\n    \"g-354\",\n    \"g-355\",\n    \"g-357\",\n    \"g-359\",\n    \"g-360\",\n    \"g-364\",\n    \"g-365\",\n    \"g-366\",\n    \"g-367\",\n    \"g-368\",\n    \"g-369\",\n    \"g-374\",\n    \"g-375\",\n    \"g-377\",\n    \"g-379\",\n    \"g-385\",\n    \"g-386\",\n    \"g-390\",\n    \"g-392\",\n    \"g-393\",\n    \"g-400\",\n    \"g-402\",\n    \"g-406\",\n    \"g-407\",\n    \"g-409\",\n    \"g-410\",\n    \"g-411\",\n    \"g-414\",\n    \"g-417\",\n    \"g-418\",\n    \"g-421\",\n    \"g-423\",\n    \"g-424\",\n    \"g-427\",\n    \"g-429\",\n    \"g-431\",\n    \"g-432\",\n    \"g-433\",\n    \"g-434\",\n    \"g-437\",\n    \"g-439\",\n    \"g-440\",\n    \"g-443\",\n    \"g-449\",\n    \"g-458\",\n    \"g-459\",\n    \"g-460\",\n    \"g-461\",\n    \"g-464\",\n    \"g-467\",\n    \"g-468\",\n    \"g-470\",\n    \"g-473\",\n    \"g-477\",\n    \"g-478\",\n    \"g-479\",\n    \"g-484\",\n    \"g-485\",\n    \"g-486\",\n    \"g-488\",\n    \"g-489\",\n    \"g-491\",\n    \"g-494\",\n    \"g-496\",\n    \"g-498\",\n    \"g-500\",\n    \"g-503\",\n    \"g-504\",\n    \"g-506\",\n    \"g-508\",\n    \"g-509\",\n    \"g-512\",\n    \"g-522\",\n    \"g-529\",\n    \"g-531\",\n    \"g-534\",\n    \"g-539\",\n    \"g-541\",\n    \"g-546\",\n    \"g-551\",\n    \"g-553\",\n    \"g-554\",\n    \"g-559\",\n    \"g-561\",\n    \"g-562\",\n    \"g-565\",\n    \"g-568\",\n    \"g-569\",\n    \"g-574\",\n    \"g-577\",\n    \"g-578\",\n    \"g-586\",\n    \"g-588\",\n    \"g-590\",\n    \"g-594\",\n    \"g-595\",\n    \"g-596\",\n    \"g-597\",\n    \"g-599\",\n    \"g-600\",\n    \"g-603\",\n    \"g-607\",\n    \"g-615\",\n    \"g-618\",\n    \"g-619\",\n    \"g-620\",\n    \"g-625\",\n    \"g-628\",\n    \"g-629\",\n    \"g-632\",\n    \"g-634\",\n    \"g-635\",\n    \"g-636\",\n    \"g-638\",\n    \"g-639\",\n    \"g-641\",\n    \"g-643\",\n    \"g-644\",\n    \"g-645\",\n    \"g-646\",\n    \"g-647\",\n    \"g-648\",\n    \"g-663\",\n    \"g-664\",\n    \"g-665\",\n    \"g-668\",\n    \"g-669\",\n    \"g-670\",\n    \"g-671\",\n    \"g-672\",\n    \"g-673\",\n    \"g-674\",\n    \"g-677\",\n    \"g-678\",\n    \"g-680\",\n    \"g-683\",\n    \"g-689\",\n    \"g-691\",\n    \"g-693\",\n    \"g-695\",\n    \"g-701\",\n    \"g-702\",\n    \"g-703\",\n    \"g-704\",\n    \"g-705\",\n    \"g-706\",\n    \"g-708\",\n    \"g-711\",\n    \"g-712\",\n    \"g-720\",\n    \"g-721\",\n    \"g-723\",\n    \"g-724\",\n    \"g-726\",\n    \"g-728\",\n    \"g-731\",\n    \"g-733\",\n    \"g-738\",\n    \"g-739\",\n    \"g-742\",\n    \"g-743\",\n    \"g-744\",\n    \"g-745\",\n    \"g-749\",\n    \"g-750\",\n    \"g-752\",\n    \"g-760\",\n    \"g-761\",\n    \"g-764\",\n    \"g-766\",\n    \"g-768\",\n    \"g-770\",\n    \"g-771\",\n    \"c-0\",\n    \"c-1\",\n    \"c-2\",\n    \"c-3\",\n    \"c-4\",\n    \"c-5\",\n    \"c-6\",\n    \"c-7\",\n    \"c-8\",\n    \"c-9\",\n    \"c-10\",\n    \"c-11\",\n    \"c-12\",\n    \"c-13\",\n    \"c-14\",\n    \"c-15\",\n    \"c-16\",\n    \"c-17\",\n    \"c-18\",\n    \"c-19\",\n    \"c-20\",\n    \"c-21\",\n    \"c-22\",\n    \"c-23\",\n    \"c-24\",\n    \"c-25\",\n    \"c-26\",\n    \"c-27\",\n    \"c-28\",\n    \"c-29\",\n    \"c-30\",\n    \"c-31\",\n    \"c-32\",\n    \"c-33\",\n    \"c-34\",\n    \"c-35\",\n    \"c-36\",\n    \"c-37\",\n    \"c-38\",\n    \"c-39\",\n    \"c-40\",\n    \"c-41\",\n    \"c-42\",\n    \"c-43\",\n    \"c-44\",\n    \"c-45\",\n    \"c-46\",\n    \"c-47\",\n    \"c-48\",\n    \"c-49\",\n    \"c-50\",\n    \"c-51\",\n    \"c-52\",\n    \"c-53\",\n    \"c-54\",\n    \"c-55\",\n    \"c-56\",\n    \"c-57\",\n    \"c-58\",\n    \"c-59\",\n    \"c-60\",\n    \"c-61\",\n    \"c-62\",\n    \"c-63\",\n    \"c-64\",\n    \"c-65\",\n    \"c-66\",\n    \"c-67\",\n    \"c-68\",\n    \"c-69\",\n    \"c-70\",\n    \"c-71\",\n    \"c-72\",\n    \"c-73\",\n    \"c-74\",\n    \"c-75\",\n    \"c-76\",\n    \"c-77\",\n    \"c-78\",\n    \"c-79\",\n    \"c-80\",\n    \"c-81\",\n    \"c-82\",\n    \"c-83\",\n    \"c-84\",\n    \"c-85\",\n    \"c-86\",\n    \"c-87\",\n    \"c-88\",\n    \"c-89\",\n    \"c-90\",\n    \"c-91\",\n    \"c-92\",\n    \"c-93\",\n    \"c-94\",\n    \"c-95\",\n    \"c-96\",\n    \"c-97\",\n    \"c-98\",\n    \"c-99\",\n]\n\ntrain = pd.read_csv(\"../input/lish-moa/train_features.csv\")\ntrain_targets = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\")\ntest = pd.read_csv(\"../input/lish-moa/test_features.csv\")\nsample_submission = pd.read_csv(\"../input/lish-moa/sample_submission.csv\")\n\ntrain, train_targets, test = mapping_and_filter(train, train_targets, test)\ntrain, test = fe_stats(train, test)\ntrain, test = c_squared(train, test)\ntrain, test = fe_pca(train, test, n_components_g=70, n_components_c=10, SEED=123)\ntrain, test, features = scaling(train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run_submission(model_dir=\"../input/moadnnfeatureengineering-20201023\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}